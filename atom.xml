<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Begriffs.com blog</title>
    <link href="http://begriffs.com/atom.xml" rel="self" />
    <link href="http://begriffs.com" />
    <id>http://begriffs.com/atom.xml</id>
    <author>
        <name>Joe Nelson</name>
        <email>cred+blog@begriffs.com</email>
    </author>
    <updated>2015-04-26T00:00:00Z</updated>
    <entry>
    <title>Three Constants to Change the Web Browser</title>
    <link href="http://begriffs.com/posts/2015-04-26-three-constants-browser.html" />
    <id>http://begriffs.com/posts/2015-04-26-three-constants-browser.html</id>
    <published>2015-04-26T00:00:00Z</published>
    <updated>2015-04-26T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Three Constants to Change the Web Browser</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">April 26, 2015</h5>
</div>

<div class="content">
  <p><a href="https://lmeyerov.github.io/">Leo Meyerovich</a>, CEO of <a href="http://www.graphistry.com/">Graphistry</a> asks, “Now that we’re living in the future, what is going to change about the web browser?” In this video he shows that powerful client GPUs, faster network latency, and decreasing cost of data-center GPUs allow a very different world on the web. Leo proposes splitting the browser between a highly parallel local rendering engine and data-center “co-processors.” This split allows supercomputer performance for data visualizations.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/125833887.hd.mp4?s=695de99f9ebdddccda7686336dd59d7a"
         poster="https://i.vimeocdn.com/video/516130137.png?mw=700"
  ></video>
</div>
</div>
]]></summary>
</entry>
<entry>
    <title>Choosing 功夫</title>
    <link href="http://begriffs.com/posts/2015-04-25-choosing-gongfu.html" />
    <id>http://begriffs.com/posts/2015-04-25-choosing-gongfu.html</id>
    <published>2015-04-25T00:00:00Z</published>
    <updated>2015-04-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Choosing 功夫</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">April 25, 2015</h5>
</div>

<div class="content">
  <p>Sitting against the wall in my apartment is a carved wooden cabinet, about three feet tall and designed for holding musical scores. I bought it from the back of a Chinese restaurant and brought it home to store CDs. The cabinet stands quietly on its little curved legs like a demure traveler from the past, a bit confused by its new contents. The contrast between the inscrutable disks and their container is the central image of this essay.</p>
<p>Compact disks (already an antiquated technology) are alien to my senses. Their information is as invisible to me as cosmic rays. All I can perceive is a rainbow scattering of light from the surface of the disk, which is technically a disturbance. I’m like a fish that can only see the disturbance of waves, unable to intuit the existence of a boat. Even if I were able to see the microscopic pits encoding the digital signal they would be visually just a nonsense jumble, the speed of their audio sampling rate is fathomless to my mind.</p>
<p>I require a tool to translate the information, and like so many tools today this one has a pushbutton interface: just press play. In one way this is a triumph of user design. I should be so lucky as to press a button and hear a song, or be lifted to another floor in a building, or make words appear in a document, or create heat in an oven, or flush a toilet, or light up a room, or divide numbers, or tell a bus to stop.</p>
<p>However in another way the pushbutton, especially in music, offers a choice. In Chinese there is a word 功夫, gongfu, which we have anglicized to Kung Fu. It actually doesn’t mean cheesy fighting moves, it means the studies or practices which take time, patience, and energy to master. Internalized knowledge that can’t be rushed. “There is no royal road to geometry,” as Euclid once explained.</p>
<p>Gongfu doesn’t change the outside world, it changes <em>us</em>, and that is why it can’t be rushed. The mind works by delicate neural habits that take time to wear into the brain. Our biggest challenge in a plentiful society is choosing our gongfu. Conveniences like recorded music present us this choice.</p>
<p>I’m grateful for audio recordings. They have expanded my musical knowledge far beyond what I might have gleaned from random local live performances. The danger, however, is that I fall back on recordings as a crutch and never develop real appreciation through playing music. The composer and conductor John Philip Sousa lived while audio recording was going mainstream and he watched the changes brought about by this technology. In <a href="http://explorepahistory.com/odocument.php?docId=1-4-1A1">The Menace of Mechanical Music</a> he worried that</p>
<blockquote>
<p>The child becomes indifferent to practice, for when music can be heard in the homes without the labor of study and close application, and without the slow process of acquiring a technic, it will be simply a question of time when the amateur disappears entirely, and with him a host of vocal and instrumental teachers, who will be without field or calling.</p>
</blockquote>
<p>Thankfully this is not what I noticed this evening. I decided to visit the <a href="http://sfcmc.org/">San Francisco Community Music Center</a> on 21st and Capp St. It was bustling with people of all ages practicing. I actually could not find a piano practice room available at the time of night I visited.</p>
<p>There are wonderful materials for people to get started learning and playing songs. The San Francisco public library has a vast collection of scores. I was able to find a fairly comprehensive folio of piano music by Federico Mompou. I checked out the score, wrote down a little reference on a notecard of the bass- and treble-cleff notes to help me read the notation more easily, and went to jump right into one of the simpler songs (Musica Callada). I’m lucky that I naturally enjoy many songs that don’t appear to require much virtuousity to play.</p>
<p>Learning anything has an opportunity cost. I could settle as I have thus far in life with enjoying recorded music. I’d have more time for coding that way. But we all have a choice of self development, of the things we want to keep in our minds. Becoming intimate with a song, actually moving our hands to play it in exactly the same way as its composer once did, this is a way to feel more empathy. All recordings suffer from the opaqueness of ease. The sounds emerge with equal ease in songs of varied difficulty. William James discussed this perception in <em>Psychology, a Briefer Course</em>:</p>
<blockquote>
<p>Only what we partly know inspires us with a desire to know more. The more elaborate textile fabrics, the vaster works in metal, to most of us are like the air, the water, and the ground, absolute existences which awaken no ideas. It is a matter of course that an engraving or a copper-plate inscription should possess that degree of beauty. but if we are shown a <em>pen</em>-drawing of equal perfection, our personal sympathy with the difficulty of the task makes us immediately wonder at the skill.</p>
</blockquote>
<p>The 功夫 tightrope exists in many fields, for instance Chess. In one way of looking at it Chess is a “solved problem.” Computers are now the world champions and operate by blindly Minimaxing a move tree. There is a beauty in understanding the problem in these terms to create a machine that can beat grandmasters. But you can also feel undaunted as a human and realize that the experience of being skilled at chess holds meaning and drama. An internal combustion engine travels faster than a bicyclist but we haven’t canceled the Tour de France.</p>
<p>Same goes for drawing or painting. The best external results are swiftly achieved with a camera (another pushbutton interface). If I were documenting a crimescene I would obviously use a camera to take copious pictures. But there are internal results from manual artistic reproduction. It is the lingering in a moment for its own sake. It’s as if in the pushbutton world we see ourselves in the third person, but when following fools’ errands like Chess or drawing we finally inhabit ourselves.</p>
<p>Even the written word has been criticized for how it modifies our minds. A dialog of Socrates points out how we can fall back on readymade words to disguise our ignorance.</p>
<blockquote>
<p>And in this instance, you who are the father of letters, from a paternal love of your own children have been led to attribute to them a quality which they cannot have; for this discovery of yours will create forgetfulness in the learners’ souls, because they will not use their memories; they will trust to the external written characters and not remember of themselves. The specific which you have discovered is an aid not to memory, but to reminiscence, and you give your disciples not truth, but only the semblance of truth; they will be hearers of many things and will have learned nothing; they will appear to be omniscient and will generally know nothing; they will be tiresome company, having the show of wisdom without the reality.</p>
</blockquote>
<p>That’s why there are degrees and types of gongfu. I am willing to weaken my memory in exchange for the richness of recorded history. The important point is to know the tradeoff.</p>
<p>And that’s why I want to begin playing piano songs, to break through the <em>musique d’ameublement</em> and start filling my cabinet with scores and not just CDs.</p>
</div>
]]></summary>
</entry>
<entry>
    <title>To Stalk a Muni</title>
    <link href="http://begriffs.com/posts/2015-04-22-to-stalk-a-muni.html" />
    <id>http://begriffs.com/posts/2015-04-22-to-stalk-a-muni.html</id>
    <published>2015-04-22T00:00:00Z</published>
    <updated>2015-04-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>To Stalk a Muni</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">April 22, 2015</h5>
</div>

<div class="content">
  <div class="figure">
<img src="/images/bus-led.png" alt="Bus Prediction LED" /><p class="caption">Bus Prediction LED</p>
</div>
<p>You’re waiting for the bus. The prediction monitor at the stop says five minutes, four, three… then just as you’re getting excited it changes its estimation to half an hour. Perhaps it’s raining and dusk and everyone is getting angry. Why are you standing here, why did the prediction API tell everyone it was time to get out to the bus stop? Can predictions be improved?</p>
<p>I’ve decided to find out. The first step to more accurate muni predictions is to accumulate historical data. With weeks’ worth of detailed bus position data we can find the patterns which cause errors in the traditional prediction algorithm.</p>
<p>Happily the realtime position and prediction API is <a href="https://www.nextbus.com/xmlFeedDocs/NextBusXMLFeed.pdf">available</a> for free and without so much as a consumer key. The historical information is not available but we can write some code to collect it.</p>
<p>Here’s where Amazon EC2 is our friend. We can spin up a micro instance, set a cron task, and then relax as it does the work. An easy and reproducible way to build an EC2 machine for the job is to use <a href="https://packer.io/">Packer</a> and <a href="https://www.chef.io/chef/">Chef</a>. We want our machine to hit the Nextbus API on a cron and save the results to S3 for later processing. That means we have to install the aws command line tool for S3 uploading, include our script and set the cron. I’ve taken care of all this at <a href="https://github.com/begriffs/stalk27">begriffs/stalk27</a> and the repo includes instructions to deploy so you can try it yourself.</p>
<p>My data collection philosophy is to save as much information as possible when sampling the API. Rather than picking relevant parts out of the XML document I’ve opted to save the whole thing. In fact I save the entire HTTP response header payload along with the document body. I think of data collection like a Mars mission – we can never go back in time with this data source so we should err on the side of over-collecting.</p>
<p>The next consideration is to choose dumb reliable storage and decouple the storage from the worker machine. Rather than save into a fancy database I just create a suitably-named file for each response. Also I avoid the local filesystem. The output is independent from the lifespan of the worker. While the worker runs it collects, else the data merely persists.</p>
<p>The only missing piece is monitoring and alerting. If this project were crucial and truly intolerant of missing data it should have some failover, redundancy and monitoring. For those kind of requirements I would choose <a href="https://github.com/begriffs/microservice-template">begriffs/microservice-template</a> and enable Amazon CloudWatch.</p>
<p>After two weeks I will begin analysis and will share with you what the data reveals about bus delays. I am collecting data on route 27 because I’ve had bad experiences waiting for wildly late buses at Powell station. If the new prediction algorithm ends up working well then I’ll scale up the data collection to more routes and maybe create a web interface for other people to check accurate departure times.</p>
</div>
]]></summary>
</entry>
<entry>
    <title>Going "Write-Only"</title>
    <link href="http://begriffs.com/posts/2015-04-20-going-write-only.html" />
    <id>http://begriffs.com/posts/2015-04-20-going-write-only.html</id>
    <published>2015-04-20T00:00:00Z</published>
    <updated>2015-04-20T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Going "Write-Only"</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">April 20, 2015</h5>
</div>

<div class="content">
  <p><a href="http://joey.hess.usesthis.com/">Joey Hess</a> inspires me. He lives in a cabin and programs Haskell on a drastically under-powered netbook. He harvests all his electricity from the sun with a homemade electrical system. Joey has designed his computing life to be highly distributed like the git version control he loves. His internet connection is intermittent and slow, his screen resolution incredibly small, but I think these seeming handicaps are actually the key to his maintaining perspective and accomplishing the mission – in his own words – of “building worthwhile things that might last.”</p>
<p>Think of what that really means. Things that last. It’s all the more surprising because Joey is talking about writing software. Most code is notoriously ephemeral. It is complicated and ties in tightly with its environment. When the environment changes even a little the code becomes unusable without maintenance. By contrast the written word, math, music, or visual art are shared between human minds (much more forgiving systems, minds) and don’t tarnish as easily as code.</p>
<p>The programmer known as “_why the lucky stiff” shares this concern:</p>
<blockquote>
<p>To program any more was pointless. My programs would never live as long as [Kafka’s] The Trial. A computer will never live as long as The Trial. … What if Amerika was only written for 32-bit Power PC? Can an unfinished program be reconstructed? Can I write a program and go, “Ah, well, you get the gist of it.” … But no. It wasn’t written for 32-bit Power PC. It was written for eyes.</p>
</blockquote>
<p>These people’s thoughts are not idle for me. They contain a reproach, a warning that one can be very busy and yet do unproductive things, hamartia. I want to focus on doing the right thing. Actually focus is the wrong word. Focusing my thoughts would imply the same thoughts but sharper, whereas I want to change the way I think.</p>
<p>Thoreau described this mental recaliberation in <a href="http://thoreau.eserver.org/life2.html">Life Without Principle</a>:</p>
<blockquote>
<p>If we have thus desecrated ourselves, — as who has not? — the remedy will be by wariness and devotion to reconsecrate ourselves, and make once more a fane of the mind. We should treat our minds, that is, ourselves, as innocent and ingenuous children, whose guardians we are, and be careful what objects and what subjects we thrust on their attention. Read not the Times. Read the Eternities.</p>
</blockquote>
<p>Whereas the above writers were all reclusive, I enjoy being around people and would like to blend a city environment with a consecration of the mind. Can I be resolute yet approachable? Perhaps yes, and perhaps it is a greater feat than solitary authenticity. Emerson <a href="https://www.gutenberg.org/files/16643/16643-h/16643-h.htm#SELF-RELIANCE">remarked</a> that</p>
<blockquote>
<p>It is easy in the world to live after the world’s opinion; it is easy in solitude to live after our own; but the great man is he who in the midst of the crowd keeps with perfect sweetness the independence of solitude.</p>
</blockquote>
<p>My experiment, then, is to arrange my life this month (and possibly beyond) to avoid contact with cheap ephemeral things and become steeped in quality thought and art. How do I know what to ignore, you ask? When in doubt I’ll leverage the survivorship bias: most old things that people still talk about are likely to be quality; the dumb old things are forgotten.</p>
<p>The simple test of time does yield false negatives. People are doing amazing work every day. But creativity needn’t be apprised of every leaf that falls. Donald Knuth said it best,</p>
<blockquote>
<p>Email is a wonderful thing for people whose role in life is to be on top of things. But not for me; my role is to be on the bottom of things. What I do takes long hours of studying and uninterruptible concentration. I try to learn certain areas of computer science exhaustively; then I try to digest that knowledge into a form that is accessible to people who don’t have time for such study.</p>
</blockquote>
<p>My first concrete step will be to eliminate variable information rewards from my computing life. These are repeated activities which occasionally – and unpredictably – give a pleasant surprise. A nontechnical example is playing slot machines. In my case it would be checking email or social media. Email is a wonderful thing but its unregulated variable rewards create compulsion. For the experiment I will check email once per week, on Monday. I’ll enable an email auto-responder for the other days to explain the setup so that nobody thinks I’m ignoring them in particular.</p>
<p>On Twitter and Github I’ll be entirely write-only. I’ll check replies/messages/issues on Mondays along with my email. For Twitter I’ll use the <a href="https://github.com/sferik/t">t</a> command line client to make posting write-only updates easy and non-distracting.</p>
<p>I will eliminate all use of the computer that is not directly related to creating things. If I’m not coding, writing, or editing videos then there will be literally nothing to do. I am going to dissociate the computer from mindless fun, from the capacity to kill time online. Without the (false?) feeling of connectedness through a glowing rectangle think how quiet and dull it can be sitting inside. Ultimately I’ll be forced by boredom to read or go outside.</p>
<p>Who can justify soaking up random online news at the expense of a neverending tidal wave of fine works from the the ocean of history? I will keep outstanding books on hand at all times, books from reading lists like <a href="http://www.sjc.edu/academic-programs/undergraduate/seminar/annapolis-undergraduate-readings/">St John’s College</a> and <a href="http://www.shimer.edu/live/files/105-academic-catalog-20132015--final-61913pdf">Shimer</a>. This way I can sublimate my urge to browse the web by instead making progress in some of the fullest and best expressions of ideas.</p>
<p>The self-censorship goes far beyond choosing classic books over web browsing. When I code I often listen to music, and music influences attitude. Choice of music affects the mind. Plato said in the Republic that,</p>
<blockquote>
<p>…rhythm and harmony most of all insinuate themselves into the inmost part of the soul and lay hold of it in bringing grace with them; and they make a man graceful if he is correctly reared, if not, the opposite.</p>
</blockquote>
<p>The discussion in The Republic maintains that would-be citizens of the ideal republic should be exposed to music that cultivates their good qualities, and prohibited from listening to the bad. Much modern music creates agitation and aggression. I’ll listen to serene and balanced songs like Gregorian chant and neoclassical, preferably from my own recordings rather than online streaming.</p>
<p>To further insulate myself from the pernicious influence of online mediocrity I will disable image loading in my browser. Online pictures are of two kinds: mundane photographs and a simplified telegraphic advertising style, for logos and minimal ornament. The “product style” is made of highly saturated homogeneous clean shapes and serves merely as simple mnemonics for products or services. The craftsmanship is these images is intentionally low because the images are meant to recall a product as efficiently as possible.</p>
<p>Extended time on the internet inures us to constraint and simplification. Take your average startup company logo and compare it with the free expression of a skilled painter. The logo looks cheap and comical, a petty token designed to evoke unthinking trust and visual association. It is easier than ever before in history to publish information worldwide, but paradoxically we produce lower quality work. Or maybe it’s the survivorship bias again. Think of a nineteenth century book with ornate etched print illustrations. The etching, and even the pressing and collating of pages were difficult processes but somehow the artists outdid us, we who can so easily create, modify and distribute images. Goodbye cartoonish web images, let me be immersed in nature and see uninhibited art instead.</p>
<p>These little rules constitute a kind of “morality,” as Nietzsche described it, a disciplinary means to attain strength.</p>
<blockquote>
<p>There should be long obedience in the same direction, there thereby results, and has always resulted in the long run, something which has made life worth living.</p>
</blockquote>
<p>I choose living among quality old things for the sake of creation, hoping by their example to attain quality of my own. Check back on this blog to see how it goes for me – unless of course I’ve inspired you to go write-only too.</p>
</div>
]]></summary>
</entry>
<entry>
    <title>SF CloudCamp Lightning Talks</title>
    <link href="http://begriffs.com/posts/2015-04-18-cloudcamp-sf-lightning-talks.html" />
    <id>http://begriffs.com/posts/2015-04-18-cloudcamp-sf-lightning-talks.html</id>
    <published>2015-04-18T00:00:00Z</published>
    <updated>2015-04-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>SF CloudCamp Lightning Talks</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">April 18, 2015</h5>
</div>

<div class="content">
  <ol style="list-style-type: decimal">
<li><a href="#intro-to-docker">Intro to Docker</a></li>
<li><a href="#exploring-strategies-for-minimal-containerization">Exploring Strategies for Minimal Containerization</a></li>
<li><a href="#storage-containers-and-microservices">Storage, Containers, and Microservices</a></li>
<li><a href="#docker-networking-with-clocker-and-project-calico">Docker Networking with Clocker and Project Calico</a></li>
<li><a href="#continuous-integration-with-docker">Continuous Integration with Docker</a></li>
</ol>
<h3 id="intro-to-docker">Intro to Docker</h3>
<p><a href="http://nathanleclaire.com/">Nathan LeClaire</a>, an engineer at Docker explains the tool from first principles as it were.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/125381777.hd.mp4?s=9e70068e605126b306f77cb0be7d8a7a"
         poster="https://i.vimeocdn.com/video/515466205.png?mw=700"
  ></video>
</div>
<ul>
<li>There is a lot of noise and media about Docker, but what is it fundamentally?</li>
<li>The precursor to containers came from Google circa 2005 managing giant workloads of linux processes
<ul>
<li>A very good paper was just released about this, <a href="https://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/43438.pdf">Large-scale cluster management at Google with Borg</a></li>
<li>So you’re Google in 2005 and you have the abstraction of unix processes</li>
<li>How do you deal with noisy neighbors - aka processes which gobble up resources</li>
<li>Their answer was patching the Linux kernel to improve process isolation</li>
<li>Resources are isolated with so-called namespaces, and can assign priority</li>
</ul></li>
<li>All this tech was contributed to the Linux kernel and evolved over time into a project called LXC
<ul>
<li>However it was hard to use and inaccessible</li>
<li>Then Dockcloud came along and made a platform-as-a-service</li>
<li>Their PaaS did not do so hot, but it did wrap LXC in a clean way</li>
</ul></li>
<li>Docker’s is best thought of as an <em>interface</em>, not a container.
<ul>
<li>It took a bunch of esoteric things and wrapped them up in a fun interface</li>
<li>It sets the stage for repeatability and isolation in your infrastructure</li>
</ul></li>
</ul>
<h3 id="exploring-strategies-for-minimal-containerization">Exploring Strategies for Minimal Containerization</h3>
<p>Presented by Brian “<a href="http://www.brianredbeard.com/">Redbeard</a>” Harrington.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/125385727.hd.mp4?s=175a6d052a48e690466d3c2c3b5f3dfa"
         poster="https://i.vimeocdn.com/video/515477269.png?mw=700"
  ></video>
</div>
<ul>
<li>Kubernetes is a fascinating project</li>
<li>It is based on the internal ideas of Google Borg and, later, Omega</li>
<li>Google doesn’t traditionally release open source projects, but they did for Kubernetes</li>
<li>Decomposing the technology
<ul>
<li>Applications and containers</li>
<li>There are a number of answers to this, including Docker</li>
<li>CoreOS made a competing product called Rocket, aka “rkt”</li>
</ul></li>
<li>The CoreOS folks have been informed by Omaha, a system for self-updating programs
<ul>
<li>Omaha is the mechanism that updates Chrome and Google Earth</li>
</ul></li>
<li>When you combine containerization, auto-updating and CoreOS you get <a href="https://tectonic.com/">Tectonic</a>, a version of Kubernetes designed to run on platforms other than Google Compute Engine</li>
</ul>
<h3 id="storage-containers-and-microservices">Storage, Containers, and Microservices</h3>
<p><a href="https://twitter.com/lmarsden">Luke Marsden</a> CTO, Founder at ClusterHQ explains how his company is working to improve the interaction and persistence of storage with Docker.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/125409828.hd.mp4?s=b19c6ea1230404dbaadc5eb4e88e71e4"
         poster="https://i.vimeocdn.com/video/515499474.png?mw=700"
  ></video>
</div>
<ul>
<li>Everybody loves containers because packaging is now so easy</li>
<li>We used to have a quagmire of install scripts and packages</li>
<li>And pushing the images to production is easy with today’s orchestration systems like Kubernetes, Mesos, Swarm, Deis etc</li>
<li>But what about state? How do you store databases stored inside a container like the rest of your application?
<ul>
<li>It would be good to put databases through the continuous integration and continuous deployment pipelines like the rest of your services</li>
<li>Docker Volumes provides only file-based access whereas most storages services are block-level, like EBS</li>
<li>You can’t manage block devices from within your containers</li>
</ul></li>
<li>Zoning
<ul>
<li>It can be difficult to move data between availability zones</li>
</ul></li>
<li>Not all orchestration managers are stateful
<ul>
<li>They usually subscribe to the 12-factor app manifesto</li>
<li>So data services have to be an external thing not managed by your orchestration system</li>
<li>Kubernetes can manage state with storage drivers, but they ignore local storage</li>
</ul></li>
<li>What would be cool is to have a “Docker for storage,” a unified REST interface
<ul>
<li>ClusterHQ is trying to solve this problem. They are building an open source project called <a href="https://github.com/ClusterHQ/flocker">Flocker</a></li>
</ul></li>
</ul>
<h3 id="docker-networking-with-clocker-and-project-calico">Docker Networking with Clocker and Project Calico</h3>
<p><a href="https://twitter.com/grkvlt">Andrew Kennedy</a> Distributed Systems Hacker at CloudSoft shares the various bits of technology they use at his company, notably the open source tool <a href="https://github.com/brooklyncentral/clocker">Clocker</a>.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/125411064.hd.mp4?s=00c723162f77b8e68695572418948c38"
         poster="https://i.vimeocdn.com/video/515501140.png?mw=700"
  ></video>
</div>
<ul>
<li>Clocker manages a Docker cloud, manages VMs, and installs Docker on them using whatever SDN (software defined networking) you want</li>
<li>Multi-host multi-container apps with seamless networking</li>
<li>The basic cloud management system is Apache Brooklyn
<ul>
<li>Clocker adds Docker blueprinting to Brooklyn</li>
<li>They use Brooklyn, jclouds, weave, opendove, and calico</li>
</ul></li>
</ul>
<h3 id="continuous-integration-with-docker">Continuous Integration with Docker</h3>
<p><a href="https://twitter.com/davenielsen">Dave Nielson</a> the Co-Founder of CloudCamp and organizer of this particular event also gives a lightning talk to explain a strategy of combining the best properties of Platforms-as-a-service with containers.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/125385728.hd.mp4?s=637d5b8d46a95703612941d609c2377b"
         poster="https://i.vimeocdn.com/video/515473232.png?mw=700"
  ></video>
</div>
<ul>
<li>Why am I such a big fan of the PaaS?</li>
<li>When you do server management yourself, maybe you do a good job, but when you do it really well why keep it to yourself?</li>
<li>Containerization kind of changed how people do cloud computing</li>
<li>However containers can accumulate and take work</li>
<li>It can also be forbidding to the command-line averse</li>
<li>Mobile app creators are a different breed
<ul>
<li>Apps either blow up or die so fast that app builders don’t care too much about the longevity of their stack or about vendor lock-in</li>
<li>“Us: watch out you’ll get locked in! Them: Oh yeah? 18 billion dollars!”</li>
</ul></li>
<li>We can get the benefits of a Platform-as-a-Service without the lock-in by developing with a buildpack stuffed inside a container
<ul>
<li>It feels like you’re deploying to Heroku or Cloud Foundry but it’s all inside a container</li>
<li>But you don’t deploy the buildpack part to production, you deploy the app that it creates, all containerized</li>
</ul></li>
</ul>
</div>
]]></summary>
</entry>
<entry>
    <title>Deploying Predictive Models in R</title>
    <link href="http://begriffs.com/posts/2015-04-10-deploying-predictive-models-in-R.html" />
    <id>http://begriffs.com/posts/2015-04-10-deploying-predictive-models-in-R.html</id>
    <published>2015-04-10T00:00:00Z</published>
    <updated>2015-04-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Deploying Predictive Models in R</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">April 10, 2015</h5>
</div>

<div class="content">
  <p><a href="https://www.linkedin.com/pub/nick-elprin/38/a0/b3">Nick Elprin</a>, co-founder of <a href="http://www.dominodatalab.com/">Domino Data Lab</a> talks about strategies for organizing R code to make it easier to deploy and scale as low-latency web services. He then shows how these techniques work in cooperation with his platform-as-a-service which turns R functions into API calls.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/124671402.hd.mp4?s=e2a64ba66124dc454e3d6126d2001124"
         poster="https://i.vimeocdn.com/video/514501213.png?mw=700"
  ></video>
</div>
<h3 id="summary">Summary</h3>
<ul>
<li>Principles of writing R code that makes it easier to deploy and operationalize predictive models you’re building</li>
<li>An intro to Domino Data Labs (data science platform-as-a-service)</li>
<li>What languages might you use for building a predictive model? What languages for deploying a web production app?
<ul>
<li>These sets form a Venn diagram with Scala, Java, Python in the intersection</li>
<li>R is all alone in the predictive-model-only circle</li>
</ul></li>
<li>Increasingly the two worlds of predictive models and production applications need to talk to each other</li>
<li>There can also be organizational friction when a development is split between data science and software engineering
<ul>
<li>Deploying new models can have some hurdles</li>
<li>One approach is the software engineers porting an R model to, say, Java</li>
<li>Out of phase release cycles</li>
</ul></li>
<li>This is the backdrop for Domino, which wraps an R model with a clean REST interface
<ul>
<li>Allows data scientists to control their own destiny. They can deploy whenever they want.</li>
</ul></li>
<li>Demo
<ul>
<li>Predicting wine quality from chemical properties</li>
<li>Using a random forest classifier just as an example</li>
<li>Then save the classifier to a file</li>
<li>Another file, predict.r, requires this file to have been created</li>
<li>So we run the training program on the cloud</li>
<li>Then “publish” the prediction file and chosen function inside</li>
</ul></li>
<li>Design considerations
<ul>
<li>Zero downtime upgrades</li>
<li>Only switches to new model when it is ready</li>
<li>High availability (restarting dead R code)</li>
<li>Versioning, rollbacks, request logging</li>
<li>Scheduled training updates</li>
</ul></li>
<li>Best practices
<ul>
<li>Make prediction code thread-safe (don’t mutate shared state)</li>
<li>Separate logical steps of code into scripts or functions</li>
<li>Training/initialization vs prediction</li>
<li>Leverage serialization tools</li>
<li>R’s save() function is a great way to do this as RDA files</li>
</ul></li>
<li>Use cases
<ul>
<li>recommendation systems</li>
<li>insurance</li>
<li>Lease or credit card approvals</li>
</ul></li>
<li>Domino’s blog welcomes guest contributions</li>
</ul>
</div>
]]></summary>
</entry>
<entry>
    <title>Circular Statistics of SF Bus Routes</title>
    <link href="http://begriffs.com/posts/2015-04-05-sf-bus-routes-circular-statistics.html" />
    <id>http://begriffs.com/posts/2015-04-05-sf-bus-routes-circular-statistics.html</id>
    <published>2015-04-05T00:00:00Z</published>
    <updated>2015-04-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Circular Statistics of SF Bus Routes</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">April  5, 2015</h5>
</div>

<div class="content">
  <p><a href="http://erictheise.com/">Eric Theise</a>, Senior Software Engineer at <a href="http://repairpal.com/">RepairPal</a> asks, “Which way is Inbound?” Though SF Muni routes are prefaced with the modifiers “Inbound” and “Outbound”, their use of the terms often has little to do with the commonly understood meaning. In this presentation Eric uses open data, an open source geostack, and R’s circular package to visually and statistically analyze and discuss the resulting cognitive dissonance.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/124132910.hd.mp4?s=f047700972c48aaaf9c1e0c47a848240"
         poster="https://i.vimeocdn.com/video/513748329.png?mw=700"
  ></video>
</div>
<h3 id="summary">Summary</h3>
<ul>
<li>Overview of the San Francisco Muni - good coverage, bad timing</li>
<li>All routes are couched in terms of “inbound” and “outbound” which can be counterintuitive in curvy areas of some routes</li>
<li>The origin of the term “inbound” in navigation, and some San Francisco history</li>
<li>Quantifying the notion of “inbound”
<ul>
<li>we’ll attack the problem with statistical tools</li>
<li>mean direction, circular variance, and hypothesis testing</li>
</ul></li>
<li>Circular statistics avoids ambiguity in histograms that “wrap-around” where they can appear either bimodal or unimodal depending on where the wrapping point occurs</li>
<li>Two packages in R for this: circular and CircStates
<ul>
<li>Eric does not know of any python or javascript packages to do this kind of thing</li>
<li>He is working on <a href="Sr%20Engineer,%20RepairPal">mctad.js</a> to address this</li>
</ul></li>
<li>A great resource to learn more is the book <a href="http://circstatinr.st-andrews.ac.uk/">Circular Statistics in R</a></li>
<li>To analyze the concept of inbound and outbound we need route information
<ul>
<li>Eric got the routes from the nextbus API</li>
<li>Stored it in PostgreSQL with the PostGIS extension</li>
</ul></li>
<li>What is the true geospatial centroid of downtown?
<ul>
<li>Turns out it’s the Mechanic’s Monument, created by “the Michelangelo of the West”</li>
</ul></li>
<li>Demo
<ul>
<li>Filter routes, and examine each stop</li>
<li>Find mean direction and variance, stop by stop</li>
<li>Use hypothesis testing to see if in general the bus route has a straightforward labeling of inbound and outbound</li>
<li>Outbound is less defined than inbound as a place (it’s everywhere NOT inbound)</li>
</ul></li>
<li>For more info about Bay Area pedestrian and transit history see the maps of <a href="https://twitter.com/enf">Eric Fisher</a></li>
</ul>
</div>
]]></summary>
</entry>
<entry>
    <title>Tracking Joy at Work</title>
    <link href="http://begriffs.com/posts/2015-03-15-tracking-joy-at-work.html" />
    <id>http://begriffs.com/posts/2015-03-15-tracking-joy-at-work.html</id>
    <published>2015-03-15T00:00:00Z</published>
    <updated>2015-03-15T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Tracking Joy at Work</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">March 15, 2015</h5>
</div>

<div class="content">
  <p>You can’t improve what you can’t measure, and what better to measure than an activity that consumes your waking life. Work.</p>
<p>Work, that oft-maligned but secretly important part of having meaning in life. Why are some days frustrating and others swept in effortless flow? Are bad days shared among an entire office or suffered individually?</p>
<p>I’ve created a program to help answer this question. Throughout the day at random intervals it privately sends me and my coworkers at <a href="http://looprecur.com">Loop/Recur</a> messages on Slack to see how we’re feeling. We respond on a scale of one to five and the results are displayed anonymously on a realtime dashboard.</p>
<div class="figure">
<img src="/images/happiness-dashboard.png" alt="happiness dashboard" /><p class="caption">happiness dashboard</p>
</div>
<p>Just asking this basic question reveals a lot and has other good side effects. Its simplest effect is to make us reflect on our mood. Quantifying how we feel raises us above inarticulate reaction. Also we can spot bad moods that we all share because they appear on the dashboard. It sparks helpful discussions.</p>
<p>The technique of sampling moods or other subjective reports over time is called the Experience Sampling Method and has been done since the 1970s. It’s been getting easier to conduct this research as communication and data collection become more accessible to computers. Not so long ago pagers were the best way to alert people to respond.</p>
<p>While developing the program I consulted a fascinating book called, “Experience Sampling Method: Measuring the Quality of Everyday Life.” I wanted to be acquainted with the methodology behind these kind of experiments. Whether or not you’re interested in data science I’d recommend you check this book out. It’s full of studies that talk about things that are fundamental to human happiness, from family life to work to gender differences. <img src="/images/experience-sampling.jpeg" style="float:right" /></p>
<p>The three common types of sampling are event contingent, interval contingent, and signal contingent. The first is where the participant makes reports during a certain type of event. At our software company it might be after committing code to git, or coming back from lunch. The second is when the participant makes recordings at the end of large intervals, like at the end of the day. We chose the last method, signal contingency.</p>
<p>Signal contingent sampling is where participants get random prompts to record data. It helps avoid memory biases and measures all computer-based activities without preference.</p>
<p>To run the system we have a daily scheduler. It picks three random times from a uniform distribution inside each user’s time preference window (the hours during which we each prefer to be solicited.)</p>
<p>The randomness of the sampling is interesting. Sometimes I may get solicited twice within a few minutes, other times more evenly throughout the day. We have considered enforcing a more even solicitation schedule (making it impossible that they occur too close together) but we don’t have a clear justification for the change. For the time being we’re leaving it as a uniform, sometimes lumpy, distribution.</p>
<p>We don’t yet have enough data to make statistically significant claims about how we feel, but it has sure been fun keeping an eye on the dashboard. Many companies have big screens showing web analytics, but we’re the first I’ve seen to track how we feel – and it feels good.</p>
<div class="alert alert-info" role="alert">
<h4>
Russian Translation
</h4>
Пост доступен на сайте softdroid.net: <a
href="http://softdroid.net/otslezhivanie-radosti-na-rabote">Softdroid: исследование производительности на работе</a>.
</div>
</div>
]]></summary>
</entry>
<entry>
    <title>Machine Learning at the Limit</title>
    <link href="http://begriffs.com/posts/2015-03-13-machine-learning-at-limit.html" />
    <id>http://begriffs.com/posts/2015-03-13-machine-learning-at-limit.html</id>
    <published>2015-03-13T00:00:00Z</published>
    <updated>2015-03-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Machine Learning at the Limit</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">March 13, 2015</h5>
</div>

<div class="content">
  <p>Professor <a href="http://www.cs.berkeley.edu/~jfc/">John Canny</a> presents benchmarks for the <a href="https://github.com/BIDData/BIDMach">BIDMach</a> Scala machine learning library. He and his team achieve a nearly theoretical maximum speed in this library relative to roofline analysis of the available CPU and GPU resources. They have managed to outperform entire machine learning clusters using a single computer. Check out the video below for more detailed comparisons.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/122075036.hd.mp4?s=c560edc9e62183abe4542a1f8b7a9bd2"
         poster="https://i.vimeocdn.com/video/510970022.png?mw=700"
  ></video>
</div>
<p>This talk was delivered at <a href="http://alpinenow.com/">Alpine Data Lab</a>.</p>
<a class="embedly-card" href="http://www.slideshare.net/ChesterChen/sf-big-analytics">Slides</a>
<script async src="//cdn.embedly.com/widgets/platform.js" charset="UTF-8"></script>
<h3 id="summary">Summary</h3>
<ul>
<li>Discovering the limits of machine learning performance using roofline design from computer architecture</li>
<li>Taking fully accelerated single-node algorithms and scaling them up</li>
<li>Tweaking a data model for machine learning should be quick and iterative</li>
<li>Often prototype machine learning is done in Scikit-Learn or R. It is then translated for production code by a group that often lacks machine learning expertise, which degrades the quality of results.</li>
<li>The reasons for developing a new ML toolkit (Bidmach):
<ul>
<li>GPUS now operate efficiently on sparse data</li>
<li>minibatch and stochastic gradient descent habe become a workhorse of choice for big data processing</li>
<li>easy customization of models</li>
<li>quick exploration, and sharing code between prototype and production</li>
</ul></li>
<li>To design the framework we start with roofline design and articulating the limits of CPUs etc</li>
<li>We build the high level operations on top of a GPU-accelerated matrix library</li>
<li>Roofline design establishes fundamental performance limits for a computational kernel</li>
<li>Comparison of the strengths of CPUs and GPUs</li>
<li>Comparing performance of BIDMach vs Vowpal Wabbit vs Scikit-Learn</li>
<li>For many tasks a single node running BIDMach outperforms a whole cluster running traditional libraries</li>
<li>Measurements of running time for Latent Dirichlet Allocation, random forests, single-class regression, logistic regression, multilabel regression, factor models, SVMs and clustering</li>
<li>Rooflining network bandwidth</li>
</ul>
</div>
]]></summary>
</entry>
<entry>
    <title>Better Tweets Through Data Science</title>
    <link href="http://begriffs.com/posts/2015-03-10-better-tweets-datascience.html" />
    <id>http://begriffs.com/posts/2015-03-10-better-tweets-datascience.html</id>
    <published>2015-03-10T00:00:00Z</published>
    <updated>2015-03-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Better Tweets Through Data Science</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">March 10, 2015</h5>
</div>

<div class="content">
  <div class="figure">
<img src="/images/bird-word.png" alt="bird words" /><p class="caption">bird words</p>
</div>
<p>Great tweets aren’t born; they’re made. It turns out there are practical steps anyone can take to give their tweets a boost. I’m going to share lessons I learned applying data science to Twitter to discover what sets the top performing tweets apart from the un-favorited masses. This article is basically a cookbook you can use to construct some tasty tweets no matter what their topic.</p>
<p>First things first. To have any chance at creating top tweets you need to start with interesting and useful content. Sorry, no amount of code or statistics can make up for a fundamentally boring article.</p>
<p>Let’s begin by assuming you have created or discovered something great to share. It is probably interesting for quite a few reasons. The trick is to recognize <em>which</em> reason most interests other people. If you emphasize the unpopular aspects of content it will appear that nobody is listening. In fact the first step is to do some listening yourself.</p>
<p>You certainly have some intuitions of what interests others, but we can go beyond fuzzy intuition and tap into that huge record that is the internet.</p>
<div class="alert alert-info" role="alert">
<h4>
Observation One
</h4>
What interests a large group of people today is what interested them yesterday.
</div>
<p>It’s our job to take the record of interest – the favorites, the retweets – and distill their topics. Not only the literal subjects under consideration but the psychological needs of the audience. This becomes more straightforward using the right algorithms.</p>
<h3 id="finding-topics">Finding Topics</h3>
<p>The first and most blunt tool for exploring topics is obviously the hashtag. They’re great both because people use them to self-identify topics and because there are existing tools online to judge hashtag popularity and relatedness.</p>
<p>Let’s get started, try this along with me. Pick a broad (unspecific) hashtag related to what you want to share. I’ll pick the super generic tag #data. We want to find other tags that are fairly popular and describe our content more precisely. To explore this use hashtagify.me</p>
<div class="figure">
<img src="/images/hashtag-graph.png" alt="Related hashtags" /><p class="caption">Related hashtags</p>
</div>
<p>Sometimes you’ll find that a tag isn’t used the way you expected. For instance I found that #cluster has more to do with jewelry than it does with cloud computing.</p>
<p>The #data graph above shows that we might want to investigate #BigData, or #privacy instead. But for this example let’s stick with #data. To really get to know the personality of this hashtag we’ll need data. So visit https://twitter.com/search-home and try #data (or your own choice of tag).</p>
<p>You’ll notice there is “Top” or “All” mode. We want to stay within Top because we want to learn about winners, not the others. Now we could use the Twitter API to programmatically download a bunch of data but we would have to sign up for access keys etc.</p>
<p>Let’s just do it the quick and easy way for now. Start scrolling down the search page causing new results to be loaded. Just keep scrolling for a long while until you have many pages of info.</p>
<p>Now use a quick snippet of JavaScript to grab the tweet text for processing. Pop open the JavaScript console in your browser (Cmd-Option-J in Chrome). Paste this in and press Enter:</p>
<pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="fu">$</span>(<span class="st">&#39;.tweet-text&#39;</span>).<span class="fu">map</span>(<span class="kw">function</span> () {
  <span class="kw">return</span> <span class="ot">$</span>.<span class="fu">trim</span>(<span class="fu">$</span>(<span class="kw">this</span>).<span class="fu">text</span>());
}).<span class="fu">get</span>().<span class="fu">join</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</code></pre>
<h3 id="revealing-patterns">Revealing Patterns</h3>
<p>Select all the output and save it to a file. We’re going to extract meaning from this text. Remember that we want the essential truth contained inside so we need to remove words which would distract from or skew the essence. Save and run the following shell script on your file to remove urls, competing hash tags, and @-mentions.</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">perl</span> -p -i -e <span class="st">&quot;s/@</span><span class="dt">\\</span><span class="st">w+[ ,</span><span class="dt">\\</span><span class="st">.</span><span class="dt">\\</span><span class="st">?</span><span class="dt">\&quot;</span><span class="st">:&#39;]//g&quot;</span> <span class="ot">$1</span>
<span class="kw">perl</span> -p -i -e <span class="st">&#39;s/@\w+$//g&#39;</span> <span class="ot">$1</span>

<span class="kw">perl</span> -p -i -e <span class="st">&#39;s/#\w+[ ,\.\?&quot;:]//g&#39;</span> <span class="ot">$1</span>
<span class="kw">perl</span> -p -i -e <span class="st">&#39;s/#\w+$//g&#39;</span> <span class="ot">$1</span>

<span class="kw">perl</span> -p -i -e <span class="st">&#39;s/pic.twitter[^ ]+ //g&#39;</span> <span class="ot">$1</span>
<span class="kw">perl</span> -p -i -e <span class="st">&#39;s/pic.twitter[^ ]+$//g&#39;</span> <span class="ot">$1</span>

<span class="kw">perl</span> -p -i -e <span class="st">&#39;s/https?:[^ ]+ //g&#39;</span> <span class="ot">$1</span>
<span class="kw">perl</span> -p -i -e <span class="st">&#39;s/https?:[^ ]+$//g&#39;</span> <span class="ot">$1</span></code></pre>
<p>We’ll use the R programming language to run topic detection through Latent Dirichlet Allocation (LDA). Here are the pieces of code to calculate them.</p>
<p>First load the tweets, one on each line. They will each be considered small “documents” for topic detection, and are put together into a “corpus.”</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(jsonlite)
<span class="kw">library</span>(tm)

tweets &lt;-<span class="st"> </span><span class="kw">strsplit</span>(<span class="kw">readLines</span>(<span class="st">&#39;/path/to/tweets.txt&#39;</span>), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)
corp &lt;-<span class="st"> </span><span class="kw">Corpus</span>(<span class="kw">VectorSource</span>(tweets))</code></pre>
<p>To minimize inconsequential variations we will remove stop words, small words and other detritus. The remaining words are stemmed.</p>
<pre class="sourceCode r"><code class="sourceCode r">dtm.control &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">tolower =</span> <span class="ot">TRUE</span>,
                    <span class="dt">removePunctuation =</span> <span class="ot">TRUE</span>,
                    <span class="dt">removeNumbers =</span> <span class="ot">TRUE</span>,
                    <span class="dt">stopwords =</span> <span class="kw">c</span>(<span class="kw">stopwords</span>(<span class="st">&quot;SMART&quot;</span>),
                                  <span class="kw">stopwords</span>(<span class="st">&quot;en&quot;</span>)),
                    <span class="dt">stemming =</span> <span class="ot">TRUE</span>,
                    <span class="dt">wordLengths =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="st">&quot;inf&quot;</span>),
                    <span class="dt">weighting =</span> weightTf)</code></pre>
<p>Next we record the term occurrences per document in a sparse matrix and set up parameters. This is a “bag-of-words” approach that ignores the grammar of sentences.</p>
<pre class="sourceCode r"><code class="sourceCode r">sparse.dtm &lt;-<span class="st"> </span><span class="kw">DocumentTermMatrix</span>(corp, <span class="dt">control =</span> dtm.control)
dtm &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(sparse.dtm)
<span class="kw">class</span>(dtm) &lt;-<span class="st"> &quot;integer&quot;</span>

vocab &lt;-<span class="st"> </span>sparse.dtm$dimnames$Terms
<span class="co"># Compute some statistics related to the data set:</span>
D &lt;-<span class="st"> </span><span class="kw">length</span>(corp)  <span class="co"># number of documents</span>
W &lt;-<span class="st"> </span><span class="kw">length</span>(vocab)  <span class="co"># number of terms in the vocab</span>
doc.length &lt;-<span class="st"> </span><span class="kw">rowSums</span>(dtm)  <span class="co"># number of tokens per document</span>
N &lt;-<span class="st"> </span><span class="kw">sum</span>(doc.length)  <span class="co"># total number of tokens in the data</span>
term.frequency &lt;-<span class="st"> </span><span class="kw">colSums</span>(dtm)  <span class="co"># frequencies of terms in the corpus</span>

<span class="co"># MCMC and model tuning parameters:</span>
K &lt;-<span class="st"> </span><span class="dv">20</span>
G &lt;-<span class="st"> </span><span class="dv">5000</span>
alpha &lt;-<span class="st"> </span><span class="fl">0.02</span>
eta &lt;-<span class="st"> </span><span class="fl">0.02</span></code></pre>
<p>Using the term frequencies we attempt to find clusters or words that tend to co-occur. This is the part that really identifies topics in tweets.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit the model:</span>
<span class="kw">library</span>(lda)

lda.input &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="dv">1</span>:<span class="kw">nrow</span>(dtm), function (i) {
    docfreq &lt;-<span class="st"> </span><span class="kw">t</span>(dtm[i,])
    keepers &lt;-<span class="st"> </span>docfreq &gt;<span class="st"> </span><span class="dv">0</span>
    <span class="kw">rbind</span>( (<span class="dv">0</span>:(<span class="kw">ncol</span>(dtm)-<span class="dv">1</span>))[keepers], <span class="kw">t</span>(dtm[i,])[keepers] )
  } )

fit &lt;-<span class="st"> </span><span class="kw">lda.collapsed.gibbs.sampler</span>(<span class="dt">documents =</span> lda.input,
                                   <span class="dt">K =</span> K, <span class="dt">vocab =</span> vocab,
                                   <span class="dt">num.iterations =</span> G, <span class="dt">alpha =</span> alpha,
                                   <span class="dt">eta =</span> eta, <span class="dt">initial =</span> <span class="ot">NULL</span>, <span class="dt">burnin =</span> <span class="dv">0</span>,
                                   <span class="dt">compute.log.likelihood =</span> <span class="ot">TRUE</span>)</code></pre>
<p>Finally plug the topic model into an interactive visualization that we will examine with our human intuition.</p>
<pre class="sourceCode r"><code class="sourceCode r">theta &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(fit$document_sums +<span class="st"> </span>alpha, <span class="dv">2</span>, function(x) x/<span class="kw">sum</span>(x)))
phi &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(<span class="kw">t</span>(fit$topics) +<span class="st"> </span>eta, <span class="dv">2</span>, function(x) x/<span class="kw">sum</span>(x)))

<span class="kw">library</span>(LDAvis)

<span class="co"># create the JSON object to feed the visualization:</span>
json &lt;-<span class="st"> </span><span class="kw">createJSON</span>(<span class="dt">phi =</span> phi,
                   <span class="dt">theta =</span> theta,
                   <span class="dt">doc.length =</span> doc.length,
                   <span class="dt">vocab =</span> vocab,
                   <span class="dt">term.frequency =</span> term.frequency)

<span class="kw">serVis</span>(json)</code></pre>
<p>Time for the fun. Here’s a screenshot of the topics detected in top tweets having the #data hashtag. I have selected topic number 19 to see the frequency of the words inside. Notice I set the relevance metric slider low to focus on words that are frequent exclusively in the topic rather than frequent in the corpus as a whole.</p>
<div class="figure">
<img src="/images/tweet-topics.png" alt="LDA topics" /><p class="caption">LDA topics</p>
</div>
<h3 id="extracting-meaning">Extracting Meaning</h3>
<p>The topics can be strange. If they are too chaotic and strange it probably means the data was not sufficiently cleaned or the sample was too small. But the more you look coherent categories the more you’ll see a kind of alien logic at work picking out words with a psychological bond.</p>
<p>At this point the algorithms have done their job and it is time for us to be creative as humans. Let me walk you through how I made sense of the topics for the #data hashtag.</p>
<p>For each detected topic I’ll list the main word, the feelings I associate with the topic, and the other major words inside that guided my assessment.</p>
<ul>
<li>The first (TRIAL) topic is cajoling people to try data products. It emphasizes the ease and quickness, and how the solution will “unlock” things, make you smarter, and amaze your boss.
<ul>
<li>storage, team, record, train, unlock, sport, free, clean, bottom, device, amazing, quick, boss, smarter</li>
</ul></li>
<li>Then we have the (CAPITAL) topic, all about disrupting industries and big company speculation.
<ul>
<li>capital, disrupt, venture, checkout, seeker, relationship, patent, airline, google, bet, fresh, double, fund</li>
</ul></li>
<li>The (BITCOIN) topic deals with that currency but also with non-monetary communication
<ul>
<li>communication, blah, exvers, search, speak, link, bitcoin, block, discount, listen, word, library, chain, song, teach, topic, print</li>
</ul></li>
<li>People worry about data, and the (BREACH) topic is full of credit cards, security compromises, and how much it costs us all.
<ul>
<li>credit, hit, compromise, driver, uber, interpret, human, card, replace, commission, hardware, neutral, pool, cost</li>
</ul></li>
<li>Of course you can’t have #data without (BIG). Apparently we’re “drowning” in it and it’s a veritable “lake,” but “wow” it is so big.
<ul>
<li>industry, lake, reach, love, move, drown, session, loyalty, wow</li>
</ul></li>
<li>The (OPPORTUNITY) topic deals with vision and red-hot engineers in the valley. You can not only wrangle data, you can be a data king.
<ul>
<li>location-based, senior, apply, hadoop, develop, succeed, hire, engineer, need, wrangle, vision, creator, gather, king, cluster, kingdom, hot, always, valley, red</li>
</ul></li>
<li>Then we have (BRANDS) and their associated trust words. People want to rate them and make decisions.
<ul>
<li>rate, trust, begin, smart, comfort, brain, decision, healthcare, focus, true, specific, rapid, crime, machine, vital, care, attribute, past, player, influence, patient</li>
</ul></li>
<li>Whereas opportunity was about creative struggle and power, (ADVANTAGE) is more about reassurance. A.k.a. debunking myths, providing answers, and strategizing.
<ul>
<li>advertis, myth, valuable, password, fuel, answer, data-driven, tip, like, competitive, autom, raw, break, provid, deliv, strategi, integr, analysi, format</li>
</ul></li>
<li>Beyond the technical aspects we have (LEGAL) implications. Laws passed or killed, policities about encryption and telecommunications.
<ul>
<li>phone, law, talktalk, combine, benefit, energi, save, stolen, plan, summari, encrypt, communicate, kill, pass, appeal, backup, transit, fall, bus, attack</li>
</ul></li>
</ul>
<div class="alert alert-info" role="alert">
<h4>
Observation Two
</h4>
Topics generated by LDA reveal strange, sometimes psychological connections in tweets.
</div>
<p>Is this subjective? Very much so. But I am searching for archetypal tweet structures, and some topics can be fairly conclusive. I have run this process on several hashtags and can definitely sense their distinct personalities.</p>
<p>For instance, check out the related tag #datascience. It should be #data’s close cousin but it does reveal that there is an industry of teaching data science. It is career-oriented.</p>
<ul>
<li>The (HIRE) shows people want to work in the field but are overwhelmed and turn to instructors.
<ul>
<li>recruit, rocket, role, survey, rocket, bank, overwhelm, instructor, begin, bet, billion</li>
</ul></li>
<li>More specific (RESOURCES)
<ul>
<li>bay, behavior, education, video, school, bootcamp, hot, write</li>
</ul></li>
<li>This topic, (MAKE), is similar to the OPPORTUNITY topic in #data but is less power-oriented. It tells a story of a journey to a beautiful outcome.
<ul>
<li>journey, digital, consider, transform, data-driven, easier, qualiti, visualis, metadata, simpli, measur, beauti, outcome, effect</li>
</ul></li>
<li>Welcome to the breathless (FUTURE) where things are emerging, things to watch
<ul>
<li>social, impact, emerging, tech, database, past, watch, attract, consum, answer, chicken&amp;egg, law</li>
</ul></li>
<li>Data science has well paying (COMPETITIONS)
<ul>
<li>learn, kaggle, spend, guest, call, competition, firm, persuade, architect, rule, half, competitor</li>
</ul></li>
<li>Sure these fancy science topics sound great, but show me a practical (APPLICATION).
<ul>
<li>develop, find, practical, succeed, economi, attend, webinar, cookbook</li>
</ul></li>
<li>Apparently (MACHINE LEARNING) is where the hard math lives
<ul>
<li>math, hard, google, library, word, oxford, advanced</li>
</ul></li>
<li>What is the biggest (STORY) of the week that you don’t want to miss?
<ul>
<li>week, news, bottleneck, back, biggest, tweet, reason, miss, tell, home, demand</li>
</ul></li>
<li>Statisticians explain (HISTORY) with infographics
<ul>
<li>integral, infographic, statistician, api, century, automate, count, sexiest</li>
</ul></li>
<li>Not just a (PATTERN) but a deep one, found faster, bigger and smarter. Boom.
<ul>
<li>program, visual, deep, language, beginn, random, linear, regress, faster, explore, bigger, smarter, storytell, connect, generate</li>
</ul></li>
<li>I call it the (ANALYTICS) topic but you might also call it the magic topic. Realtime things for experts. It tracks stuff and never stops.
<ul>
<li>realtime, message, queue, test, predict, track, influence, expert, review, embrace, magic</li>
</ul></li>
</ul>
<h3 id="crafting-the-tweets">Crafting the Tweets</h3>
<p>So we found the main psychological topics in top tweets from our categories. How can we construct new irresistably sharable tweets? Use the topic words as your guide. Let’s think about this article itself. We could consider it in the cringe-worthy genre of social media marketing techniques and learn more about those hashtags, but let’s think of it as #datascience and #data.</p>
<p>This article does deal with making things (tweets) with an anticipated outcome, so let’s cast it as a MAKE topic. Review the words in the topic again to see how I used them.</p>
<blockquote>
<p>My #datadriven journey to measure and transform my tweets</p>
</blockquote>
<p>It is also an APPLICATION of a particular algorithm, so dressing it in those clothes we get something like</p>
<blockquote>
<p>A cookbook for practical #datascience to make your tweets succeed</p>
</blockquote>
<p>But it provides OPPORTUNITIES to improve, hence</p>
<blockquote>
<p>How to create successful tweets by wrangling Twitter #data with #textmining</p>
</blockquote>
<p>Of course we’re dealing with PATTERNS and we can mimic that topic with something like,</p>
<blockquote>
<p>Exploring deep language patterns to tweet smarter (#datascience)</p>
</blockquote>
<h3 id="conclusion">Conclusion</h3>
<p>We’ve seen how to explore the space of hashtags, find topics in each one, and use our intuition to find psychological motives in topics. We then used the topic words as inspiration for how to communicate with others in the language that has proven most effective.</p>
<p>Certainly a bag-of-words approach such as we’ve outlined is limited, and leaning on it for every tweet will probably just sound generic. Ultimately Twitter is a place to have normal conversations with people. However I have found that algorithmically analyzing topics helps me see other people’s point of view and write more compelling content.</p>
</div>
]]></summary>
</entry>

</feed>
