<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Begriffs.com blog</title>
    <link href="http://begriffs.com/atom.xml" rel="self" />
    <link href="http://begriffs.com" />
    <id>http://begriffs.com/atom.xml</id>
    <author>
        <name>Joe Nelson</name>
        <email>cred+blog@begriffs.com</email>
    </author>
    <updated>2015-01-28T00:00:00Z</updated>
    <entry>
    <title>Virtualizing a Hadoop Cluster (two videos)</title>
    <link href="http://begriffs.com/posts/2015-01-28-virtualizing-hadoop.html" />
    <id>http://begriffs.com/posts/2015-01-28-virtualizing-hadoop.html</id>
    <published>2015-01-28T00:00:00Z</published>
    <updated>2015-01-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Virtualizing a Hadoop Cluster (two videos)</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">January 28, 2015</h5>
</div>

<div class="content">
  <p><a href="http://joelburget.com/">Tom Phelan</a>, Chief Architect at <a href="https://www.linkedin.com/pub/tom-phelan/0/749/61b">BlueData</a> talks about the appropriate situations in which to virtualize Hadoop, either in containers or in virtual machines. In evaluating the situations he explains what questions you should and should not be asking.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/117907449.hd.mp4?s=de0ba4566372c0f6ae14d8bb51f50497"
         poster="https://i.vimeocdn.com/video/504771461.png?mw=700"
  ></video>
</div>
<h3 id="summary">Summary</h3>
<ul>
<li>What BlueData has learned about running Hadoop jobs</li>
<li>Under what situations should one virtualize a Hadoop cluster</li>
<li>The shape and components of a physical cluster
<ul>
<li>Both master and worker controllers contain</li>
<li>Disks, server, named node, and resource manager</li>
</ul></li>
<li>Types of virtualization:
<ul>
<li>public cloud</li>
<li>private cloud / hypervisor (strong fault isolation)</li>
<li>private cloud / containers (weak fault isolation)</li>
<li>paravirtualization</li>
</ul></li>
<li>The appropriate infrastructure for various situations
<ul>
<li>Questions NOT to ask</li>
<li>Questions to ask</li>
</ul></li>
<li>Performance and Data Locality</li>
<li>Five use-cases and their infrastructure needs</li>
</ul>
<h3 id="pt-2-orchestration-with-docker">Pt 2: Orchestration with Docker</h3>
<p><a href="https://twitter.com/joel_k_baxter">Joel Baxter</a>, also at BlueData, leads a breakout session about what an ideal orchestration manager would look like for managing Hadoop clusters and associated data. The state of the art is evolving but not there yet.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/118047022.hd.mp4?s=91cfdf09dcdfcc374d31c69c3008ce3a"
         poster="https://i.vimeocdn.com/video/504974721.png?mw=700"
  ></video>
</div>
<h3 id="summary-1">Summary</h3>
<ul>
<li>Hadoop Cluster orchestration with Docker</li>
<li>Wishlist for an ideal orchestration manager
<ul>
<li>Add/remove a host from a list of peers</li>
<li>Track resource consumption/availability per host</li>
<li>Library of pre-prepared application images</li>
<li>Multi-tenant protection
<ul>
<li>Performance isolation and data security</li>
<li>Containers are less secure than virtual machines</li>
<li>Quotas and priority</li>
</ul></li>
<li>Matching containers with physical hosts</li>
<li>Spreading containers across fault-zones</li>
<li>Packing containers in contiguous memory</li>
</ul></li>
<li>Migrations vs stateless servers</li>
<li>Circumventing docker and doing IP address allocation</li>
<li>DNS settings for bidirectional Hadoop communication</li>
<li>Wishlist for storage
<ul>
<li>Docker volumes</li>
<li>Volumes for fragments of HDFS are hard to reconstruct on container resurrection</li>
<li>Ship your data to another db, or paravirtualize the storage</li>
</ul></li>
<li>Solutions?
<ul>
<li>There is not yet a turn-key orchestration system that solves all the items in the wishlists</li>
<li>The area is rapidly evolving</li>
</ul></li>
</ul>
</div>

<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'begriffs';

  (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
]]></summary>
</entry>
<entry>
    <title>Writing a React JS front-end in Haskell</title>
    <link href="http://begriffs.com/posts/2015-01-12-reactjs-in-haskell.html" />
    <id>http://begriffs.com/posts/2015-01-12-reactjs-in-haskell.html</id>
    <published>2015-01-12T00:00:00Z</published>
    <updated>2015-01-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Writing a React JS front-end in Haskell</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">January 12, 2015</h5>
</div>

<div class="content">
  <p><a href="http://joelburget.com/">Joel Burget</a>, Developer at <a href="https://www.khanacademy.org/">Khan Academy</a> explains the design of his new <a href="https://hackage.haskell.org/package/react-haskell">react-haskell</a> library.</p>
<p>It allows you to write a front-end app in Haskell which you compile to JavaScript via Haste and render using React JS.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/116516127.hd.mp4?s=69cd6b66fe5bff69a43e1b4375482edc"
         poster="https://i.vimeocdn.com/video/502970736.png?mw=700"
  ></video>
</div>
<p>Check out the code at <a href="http://joelburget.com/react-haskell/">joelburget.com/react-haskell</a>.</p>
<h3 id="overview">Overview</h3>
<ul>
<li>Overview of React itself (without the Haskell)
<ul>
<li>The virtual dom</li>
<li>Partial diffing</li>
</ul></li>
<li>Haste: turning Haskell into JavaScript</li>
<li>Inspiration for React-Haskell
<ul>
<li>Blaze-HTML builder provided the general flavor of the API</li>
<li>Recently Joel has been switching to the Lucid library instead of Blaze</li>
<li>Building the eventual DOM happens inside a monad</li>
<li>It could be considered an abuse of a monad, but provides nice do-notation</li>
</ul></li>
<li>Examples of react html written in Haskell</li>
<li>Event handlers</li>
<li>Principles of interruptible animations (inspired by UIKit)
<ul>
<li>the model is discrete</li>
<li>we apply updates immediately</li>
<li>use additive animation by default</li>
</ul></li>
<li>Performance observations</li>
</ul>
</div>

<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'begriffs';

  (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
]]></summary>
</entry>
<entry>
    <title>Declaring RESTful APIs with PostgREST</title>
    <link href="http://begriffs.com/posts/2014-12-30-intro-to-postgrest.html" />
    <id>http://begriffs.com/posts/2014-12-30-intro-to-postgrest.html</id>
    <published>2014-12-30T00:00:00Z</published>
    <updated>2014-12-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Declaring RESTful APIs with PostgREST</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">December 30, 2014</h5>
</div>

<div class="content">
  <h3 id="design-philosophy">Design Philosophy</h3>
<p>In this video I explain the design philosophy for my recently-released API server, <a href="https://github.com/begriffs/postgrest">PostgREST</a>. This server creates a complete and RESTful API for any existing PostgreSQL database by inspecting the db schema.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/115668217.hd.mp4?s=7360aebb5727439aceb71202c4d4c775"
         poster="https://i.vimeocdn.com/video/501761918.png?mw=700"
  ></video>
</div>
<h3 id="begriffspostgrest"><a href="https://github.com/begriffs/postgrest">begriffs/postgrest</a></h3>
</div>

<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'begriffs';

  (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
]]></summary>
</entry>
<entry>
    <title>Intro to the Jut Dataflow Platform</title>
    <link href="http://begriffs.com/posts/2014-12-11-jut-dataflow-platform.html" />
    <id>http://begriffs.com/posts/2014-12-11-jut-dataflow-platform.html</id>
    <published>2014-12-11T00:00:00Z</published>
    <updated>2014-12-11T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Intro to the Jut Dataflow Platform</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">December 11, 2014</h5>
</div>

<div class="content">
  <p><a href="https://github.com/demmer">Michael Demmer</a>, VP Enginnering at <a href="http://www.jut.io/">Jut</a> unveils their dataflow platform which has been under development for the past year and a half.</p>
<p>In this video Mike gives some demos of the system and shares how they think about modeling timeseries data along with unstructured events in a complex system.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/114224611.hd.mp4?s=76850f410b1202d175397f1db626fc15"
         poster="https://i.vimeocdn.com/video/499957414.png?mw=700"
  ></video>
</div>
<h3 id="overview">Overview</h3>
<ul>
<li>Jut wants to tackle the problems of managing complex systems</li>
<li>Vs existing products which focus on one small slice of the pie</li>
<li>Data is diverse
<ul>
<li>We have regular metric data</li>
<li>…and also unstructured events like server logs and site actions</li>
<li>Historical and current events</li>
</ul></li>
<li>Want to take all types of data, analyze it and automate alerts</li>
</ul>
<h4 id="small-demos">Small Demos</h4>
<ul>
<li>Graphing 90th percentile server response times across various services</li>
<li>Single server response times overlayed with error events</li>
<li>Modularity of the Juttle language</li>
<li>Data joins</li>
<li>Sharing and loading Juttle gists within the Jut Playground</li>
</ul>
<h4 id="data-integration">Data Integration</h4>
<ul>
<li>How to integrate in a SaaS/on-premise hybrid</li>
<li>Logs, metrics, events generated behind a firewall can be processed back there in a Jut “engine”</li>
<li>The UI and auth lives on jut.io, and it sends commands through your firewall via a control channel</li>
</ul>
<h4 id="applications-in-practice">Applications in Practice</h4>
<ul>
<li>Getting meta: Using Jut to monitor Jut usage itself</li>
<li>Example analyzing actual SSH break-in attempts on jut.io</li>
<li>Using the same program to view and mash up events gathered in different ways</li>
<li>Memory profiling of application</li>
<li>Q&amp;A</li>
</ul>
</div>

<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'begriffs';

  (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
]]></summary>
</entry>
<entry>
    <title>Datacenter to AWS Cloud Migration</title>
    <link href="http://begriffs.com/posts/2014-12-04-datacenter-to-aws.html" />
    <id>http://begriffs.com/posts/2014-12-04-datacenter-to-aws.html</id>
    <published>2014-12-04T00:00:00Z</published>
    <updated>2014-12-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Datacenter to AWS Cloud Migration</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">December  4, 2014</h5>
</div>

<div class="content">
  <p><a href="https://twitter.com/theotypes">Theo Kim</a>, senior director of SaaS at <a href="http://www.jobvite.com/">Jobvite</a> describes the experience of migrating the entire company infrastructure from a dedicated data center to Amazon Web Services.</p>
<p>He is joined onstage by <a href="https://plus.google.com/+ScottGust/posts">Scott Gust</a> and <a href="https://plus.google.com/108124405312975902364/posts">Brian Morehead</a> to describe Jobvite’s three biggest migration challenges and their solutions.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/113586351.hd.mp4?s=fc198dd18083a28ef5d171eef732c10f"
         poster="https://i.vimeocdn.com/video/499956612.png?mw=700"
  ></video>
</div>
<h3 id="overview">Overview</h3>
<h4 id="theo-kim">Theo Kim</h4>
<ul>
<li>The pain of the old data center model</li>
<li>Thinking of services rather than servers</li>
<li>Recap of gains with AWS</li>
<li>Cutting costs with reserve- and spot-instances</li>
</ul>
<h4 id="scott-gust">Scott Gust</h4>
<ul>
<li>Three main architectural challenges
<ul>
<li>uri load balancing</li>
<li>autoscaling adoption</li>
<li>network attached storage migration</li>
</ul></li>
<li>Way to work around the elastic load balancer’s uri support
<ul>
<li>Used HAProxy behind ELB to do the matching</li>
<li>Important to insulate IP changes caused by ELB</li>
</ul></li>
<li>Injecting headers with the load balancer</li>
<li>As servers become available they recreate the HAProxy config</li>
</ul>
<h4 id="brian-morehead">Brian Morehead</h4>
<ul>
<li>The migration was fun!</li>
<li>Autoscaling made them reconsider how to automate datacenter processes</li>
<li>Moving beyond the problem of the “snowflake server”</li>
<li>Config management
<ul>
<li>All ec2 instances are constructed from a repo in s3</li>
<li>Chose puppet over chef or ansible</li>
<li>Standardizing hostname</li>
<li>LDAP auth</li>
</ul></li>
<li>Puppet installs a cron job to register machines with HAProxy</li>
<li>Monitoring with Nagios and CheckMK</li>
<li>NAS migration
<ul>
<li>At the time of migration they had 8TB of storage and 40M+ files (resumes, attachments etc)</li>
<li>How about S3? Not really a NAS, and no metadata</li>
<li>They chose Zadara Storage to avoid stitching together EBS volumes</li>
<li>But they’re moving to an all-s3 solution</li>
</ul></li>
<li>Copying files to s3 can be slow
<ul>
<li>Even multithreaded copies and bigger instances didn’t help</li>
<li>Solution: spot instances</li>
<li>Spun up 15-20 instances and it blasted the 40M files copied in three days</li>
</ul></li>
</ul>
<h4 id="theo">Theo</h4>
<ul>
<li>Results</li>
<li>Ended up with a 20% cost savings by moving off a managed service provider</li>
<li>Great uptime</li>
<li>Fast patches to security vulnerabilities</li>
<li>Gets to have Christmas and Thanksgiving with the family (!)</li>
<li>Ephemeral servers to do quick tests (load etc)</li>
<li>Cheap services
<ul>
<li>sending 275000 emails per day for $150 per month</li>
</ul></li>
<li>Mountains of metrics from CloudWatch and CloudTrail</li>
<li>Q&amp;A
<ul>
<li>Network attached storage…don’t use it!</li>
<li>Stop worrying about encryption-at-rest in the cloud</li>
<li>Data centers have a “red zone” where equipment coming out literally goes through a wood chipper</li>
<li>Intentionally undersize staging resources to force devs to make leaner code (heh heh)</li>
</ul></li>
</ul>
</div>

<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'begriffs';

  (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
]]></summary>
</entry>
<entry>
    <title>A Survey of Data Science</title>
    <link href="http://begriffs.com/posts/2014-11-30-survey-of-data-science.html" />
    <id>http://begriffs.com/posts/2014-11-30-survey-of-data-science.html</id>
    <published>2014-11-30T00:00:00Z</published>
    <updated>2014-11-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>A Survey of Data Science</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">November 30, 2014</h5>
</div>

<div class="content">
  <p>What does it take to be an outstanding data scientist? I decided to find out and reviewed syllabi from every data science bootcamp, the requirements for dozens of data jobs, and scores of slideshares and wikipedia articles. I think I’ve identified and classified the core skills.</p>
<p>Fundamentally there are six abilities of a competent data scientist.</p>
<ul>
<li>acquiring data in a highly-available distributed pipeline</li>
<li>cleaning it of duplication and errors</li>
<li>enhancing it with knowledge gained from unsupervised learning and nlp</li>
<li>inferring properties of a population from a sample, and finding relations of statistical variables</li>
<li>predicting future events through supervised learning and statistical models</li>
<li>communicating my findings to people and external programs</li>
</ul>
<p>As many articles mentioned, dealing with data is a team job and no single individual is expected to know everything. There are two distinct specializations though, data <em>science</em> and <em>engineering</em>. The latter deails with pipelines, wrangling, availability, and cluster management. The former with asking the right questions, designing experiments, and statistical analysis.</p>
<p>I compiled a list of specific technologies and techniques and fit them into the sixfold classification of abilities. These appear to be the core skills of the profession.</p>
<ul>
<li>Acquire
<ul>
<li>workers and other pieces are managed by mesos</li>
<li>web scrapers and other events as kafka producers</li>
<li>mesos runs them in docker containers with the marathon framework</li>
<li>feeds into a “lambda architecture”
<ul>
<li>the master dataset accumulates immutably in hdfs
<ul>
<li>encode the data with the thrift format to add a schema</li>
<li>thence through hadoop jobs to make precomputed views</li>
<li>store views in elephantdb and recalculate periodically</li>
<li>client access is read-only from elephant</li>
</ul></li>
<li>data forks into storm for real time “speed layer” processing
<ul>
<li>which redundantly calculates the views we are doing on hadoop</li>
<li>the results go into cassandra which is read/write</li>
<li>the realtime layer relies on incremental algorithms to update the state in that database</li>
</ul></li>
</ul></li>
</ul></li>
<li>Clean
<ul>
<li>record linking and deduping
<ul>
<li>soundex</li>
<li>jaro–winkler distance</li>
<li>unicode normalization</li>
</ul></li>
<li>outlier identification
<ul>
<li>normality testing</li>
<li>Grubbs’ test for outliers (given normality)</li>
</ul></li>
<li>correcting errors with mechanical turk</li>
<li>create a derived hdfs dataset, the “golden master”</li>
</ul></li>
<li>Enhance
<ul>
<li>feature extraction
<ul>
<li>principal component analysis</li>
<li>k-NN</li>
</ul></li>
<li>unsupervised clustering
<ul>
<li>k-means and finding k with the gap statistic</li>
<li>hierarchical (agglomerative, divisive)</li>
<li>density (DBSCAN)</li>
<li>bagging</li>
</ul></li>
<li>nlp
<ul>
<li>stemming</li>
<li>bag of words</li>
<li>tf–idf</li>
<li>topic models</li>
</ul></li>
</ul></li>
<li>Infer
<ul>
<li>point estimation
<ul>
<li>maximum likelihood</li>
</ul></li>
<li>interval estimation
<ul>
<li>credible intervals</li>
</ul></li>
<li>survival analysis
<ul>
<li>kaplan-meier estimator</li>
</ul></li>
<li>hypothesis testing
<ul>
<li>chi squared</li>
<li>t-test</li>
</ul></li>
<li>correlation</li>
</ul></li>
<li>Predict
<ul>
<li>classification
<ul>
<li>logistic regression</li>
<li>decision trees and random forests</li>
<li>naive bayes</li>
<li>variance-bias decomposition</li>
</ul></li>
<li>recommendations
<ul>
<li>collaborative filtering</li>
<li>content filtering</li>
</ul></li>
</ul></li>
<li>Communicate
<ul>
<li>descriptive statistics
<ul>
<li>measures of central tendency</li>
<li>measures of dispersion</li>
</ul></li>
<li>visualization
<ul>
<li>d3 (vega, rickshaw)</li>
<li>realtime dashboard</li>
</ul></li>
<li>exposing results in restful api
<ul>
<li>export to postgres with sqoop</li>
<li>generate api with dbapi</li>
<li>document with raml</li>
</ul></li>
</ul></li>
</ul>
<p>This seems like the state of the art although there is some dissenting opinion such as <a href="http://radar.oreilly.com/2014/07/questioning-the-lambda-architecture.html">Questioning the Lambda Architecture</a>. Ultimately I think the engineering side will be moving to a purely functional paradigm. Perhaps it could be written in Haskell, using libraries like <a href="https://hailstorm-hs.github.io/hailstorm/">hailstorm</a>, <a href="https://github.com/cosbynator/haskakafka">haskakafka</a>, and <a href="https://github.com/Soostone/hadron">hadron</a>. Everything I read agreed that Hadoop and associated Java tools are creaky beasts that require a lot of fiddling.</p>
<p>Although the engineering side looks exhilerating (unleashing hordes of data scrapers and coordinating a big pipeline), the science side may be a calmer career bet with its longer-lasting knowledge. However even there I discovered some fundamental disagreements between Frequentists and Bayesians. Either way it’s all fascinating stuff and I’d like to create some test compute clusters to up my game.</p>
</div>

<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'begriffs';

  (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
]]></summary>
</entry>
<entry>
    <title>Intro to Apache Mesos, the distributed systems SDK</title>
    <link href="http://begriffs.com/posts/2014-11-28-intro-to-apache-mesos.html" />
    <id>http://begriffs.com/posts/2014-11-28-intro-to-apache-mesos.html</id>
    <published>2014-11-28T00:00:00Z</published>
    <updated>2014-11-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Intro to Apache Mesos, the distributed systems SDK</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">November 28, 2014</h5>
</div>

<div class="content">
  <p><a href="https://twitter.com/quarfot">Niklas Nielsen</a>, distributed systems engineer at <a href="https://mesosphere.com/">Mesosphere</a> gives an overview of how <a href="https://mesos.apache.org/">Mesos</a> manages resources to ensure their fair and efficient use in a compute cluster. He also demonstrates two higher-level frameworks on top of Mesos which keep jobs alive and manage timing and dependencies.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/113052137.hd.mp4?s=7564b3ae933804f64a12ebfe42389af1"
         poster="https://i.vimeocdn.com/video/499956815.png?mw=700"
  ></video>
</div>
<h3 id="overview">Overview</h3>
<ul>
<li>What is Apache Mesos (and what is it not)</li>
<li>Abstracting from physical machines to resources</li>
<li>“Everything fails all the time”</li>
<li>Mesos’ heuristic for the NP-Hard cloud scheduling problem</li>
<li>Delegation to local decision-making nodes</li>
<li>Resource offers</li>
<li>Scheduling tasks across racks or nodes using attributes</li>
<li>Avoiding resource starvation using reservations</li>
<li>Mesos is a kernel with which you rarely interact directly
<ul>
<li>You use frameworks on top</li>
<li><strong>Marathon</strong> starts processes in a mesos cluster and does deployments and upgrades</li>
<li><strong>Chronos</strong> is a distributed cron with dependencies</li>
</ul></li>
<li>Twitter uses Mesos to handle
<ul>
<li>240 million monthly users</li>
<li>150k tweets per second</li>
<li>100TB per day of compressed data</li>
</ul></li>
</ul>
</div>

<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'begriffs';

  (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
]]></summary>
</entry>
<entry>
    <title>Robot programming in APL</title>
    <link href="http://begriffs.com/posts/2014-11-26-robots-in-apl.html" />
    <id>http://begriffs.com/posts/2014-11-26-robots-in-apl.html</id>
    <published>2014-11-26T00:00:00Z</published>
    <updated>2014-11-26T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Robot programming in APL</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">November 26, 2014</h5>
</div>

<div class="content">
  <p><a href="https://twitter.com/mkromberg">Morten Kromberg</a>, CTO at <a href="http://www.dyalog.com">Dyalog</a> shows two robots that communicate wirelessly, navigate, and explore. And they’re written in a language little changed since the 1960s, <a href="http://www.computerhistory.org/atchm/the-apl-programming-language-source-code">APL</a>.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/112894665.hd.mp4?s=468260e4674c6f6024d86d373d93473d"
         poster="https://i.vimeocdn.com/video/499956995.png?mw=700"
  ></video>
</div>
<p>Throughout the night Morten showed the hieroglyphic language doing web scraping, writing servers, 3d simulations, and runnings robots. The language has a small fervent userbase, mainly European. They’re in a fascinating parallel universe of computing, one which talks about those “new additions” to the language in the early 90s.</p>
<p>They carry the APL torch in corners of industries like finance, medical records, and (real) engineering. It’s a language often wielded by people who are knowledgeable in fields other than coding, people who program incidentally to support their main activities.</p>
</div>

<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'begriffs';

  (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
]]></summary>
</entry>
<entry>
    <title>Creating a package on Hackage</title>
    <link href="http://begriffs.com/posts/2014-10-25-creating-package-hackage.html" />
    <id>http://begriffs.com/posts/2014-10-25-creating-package-hackage.html</id>
    <published>2014-10-25T00:00:00Z</published>
    <updated>2014-10-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Creating a package on Hackage</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">October 25, 2014</h5>
</div>

<div class="content">
  <p>How do you go from a pile of Haskell code on your machine to a finished package on <a href="http://hackage.haskell.org">Hackage</a> in all its tactfully understated glory? I recently took this journey and some of the steps were tricky enough that I thought someone ought to write a guide.</p>
<h3 id="step-1---the-account">Step 1 - the account</h3>
<p><a href="http://hackage.haskell.org/users/register-request">Register</a> a user account. Like many steps in Hackage, this is a somewhat human and manual process. A person has to review your submission and deem you worthy. So I guess don’t pick a profile name like <code>javascriptFTW</code> that would anger them.</p>
<h3 id="step-2---package-structure">Step 2 - package structure</h3>
<p>Structure your package in a standard way. Source directories are nested and named after the exposed module path. Your test suite lives in its own place and needs some boilerplate configuration. And of course you’ll need a good cabal file. <em>Want the shortcut?</em> Run <a href="https://github.com/fujimura/hi">fujimura/hi</a> to generate your project structure. By default it will choose a BSD license and use <code>hspec</code> for tests. Speaking for myself I changed it to an MIT license and <a href="http://hackage.haskell.org/package/hspec2">hspec2</a>.</p>
<pre class="sourceCode bash"><code class="sourceCode bash">$ <span class="kw">hi</span> --module-name <span class="st">&quot;Foo.Bar.Baz&quot;</span> --author <span class="st">&quot;J Doe&quot;</span> --email <span class="st">&quot;jdoe@me.com&quot;</span></code></pre>
<h3 id="step-3---cabal">Step 3 - cabal</h3>
<p>Customize the package cabal file. If you’re wondering whether it includes enough information, check with</p>
<pre class="sourceCode bash"><code class="sourceCode bash">$ <span class="kw">cabal</span> check</code></pre>
<p>which will point out any problems. In fact Hackage will refuse a package upload that fails the check. However there are fields you might like to add beyond the bare minimum, such as the <code>Homepage</code> field with a link to the project on Github. See this <a href="http://www.haskell.org/ghc/docs/7.0.3/html/Cabal/authors.html#general-fields">reference</a> of all the fields.</p>
<h3 id="step-4---docs">Step 4 - docs</h3>
<p>Add Haddock documentation to your code as comments. There is more to Haddock than can reasonably fit in a little guide like this, but a good way to go is copy what people do in a popular package you admire. Generally the structure of generated docs is determined in your module declaration. Be sure to include some example code since it gives a quick overview to would-be users.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">module</span> <span class="dt">Foo.Bar.Baz</span>
  (
    <span class="co">-- * Example usage</span>
    <span class="co">-- $use</span>

    <span class="co">-- * A section</span>
    <span class="dt">Thingie</span>(<span class="fu">..</span>)

    <span class="co">-- * More stuff</span>
  , fun
  , joy
  ) <span class="kw">where</span></code></pre>
<p>Notice you can create chunks by name like <code>$use</code> and interpolate them at the right place in the module declaration. To preview your docs locally run</p>
<pre class="sourceCode bash"><code class="sourceCode bash">$ <span class="kw">cabal</span> haddock</code></pre>
<p>which will build the docs in <code>dist/doc/html/your-pkg/index.html</code>. Note that unlike the custom in other languages, Haskellers usually don’t put much info into READMEs. At first I thought they were being negligent but I learned that the combination of strong types and Haddock’s structure provide a standardized documentation experience that works better. I like to add a link in my Github readme to point at the Hackage docs to help those unfamiliar with the convention.</p>
<h3 id="step-5---ci">Step 5 - CI</h3>
<p>Enable continuous integration. You want to ensure that</p>
<ul>
<li>your usual tests pass</li>
<li>your project works in a few versions of GHC</li>
<li>your cabal file is well-formed</li>
<li>a source distribution can be generated</li>
<li>docs build cleanly</li>
</ul>
<p>Here’s a nice Travis config adapted from <a href="https://twitter.com/bitemyapp">bitemyapp</a> who got it from <a href="https://github.com/hvr">hvr</a>.</p>
<pre class="sourceCode yaml"><code class="sourceCode yaml"><span class="fu">language:</span> haskell

<span class="fu">ghc:</span>
 <span class="kw">-</span> 7.6
 <span class="kw">-</span> 7.8

<span class="fu">before_install:</span>
 <span class="kw">-</span> <span class="fu">sudo add-apt-repository -y ppa:</span>hvr/ghc
 <span class="kw">-</span> sudo apt-get update
 <span class="kw">-</span> sudo apt-get install happy-1.19.3
 <span class="kw">-</span> sudo apt-get install alex-3.1.3
 <span class="kw">-</span> <span class="fu">export PATH=~/.cabal/bin:</span>$PATH <span class="co"># for newer alex</span>
 <span class="kw">-</span> cabal update
 <span class="kw">-</span> cabal install alex happy

<span class="fu">install:</span>
 <span class="kw">-</span> cabal install --only-dependencies --enable-tests --enable-benchmarks --force-reinstalls

<span class="fu">script:</span>
 <span class="co"># -v2 provides useful information for debugging</span>
 <span class="kw">-</span> cabal configure --enable-tests --enable-benchmarks -v2

 <span class="co"># this builds all libraries and executables</span>
 <span class="co"># (including tests/benchmarks)</span>
 <span class="kw">-</span> cabal build

 <span class="kw">-</span> cabal test
 <span class="kw">-</span> cabal check

 <span class="co"># tests that a source-distribution can be generated</span>
 <span class="kw">-</span> cabal sdist

 <span class="co"># check that the generated source-distribution can be built &amp; installed</span>
 <span class="kw">-</span> export SRC_TGZ=$(cabal info . | awk <span class="st">&#39;{print $2 &quot;.tar.gz&quot;;exit}&#39;</span>) ;
   cd dist/;
   if <span class="kw">[</span> -f <span class="st">&quot;$SRC_TGZ&quot;</span> <span class="kw">]</span>; then
      cabal install <span class="st">&quot;$SRC_TGZ&quot;</span>;
   else
      echo <span class="st">&quot;expected &#39;$SRC_TGZ&#39; not found&quot;</span>;
      exit 1;
   fi</code></pre>
<h3 id="step-6---dependencies">Step 6 - dependencies</h3>
<p>Add some constraints to your library’s dependencies in the cabal file. Locking major versions of dependencies will help prevent surprise Cabal build failures by your users. This is about the only rule of thumb I know. Setting constraints more intelligently is certainly possible, and I welcome your comments about your own strategies.</p>
<h3 id="step-7---candidate-package">Step 7 - candidate package</h3>
<p>Your tests pass, your docs look good. It’s time to upload a “package candidate” for a last check that things are OK on the real Hackage.</p>
<p>Create a source distribution</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">cabal</span> sdist</code></pre>
<p>This will generate <code>dist/your-pkg-x.y.z.tar.gz</code>. Select this file in the <a href="https://hackage.haskell.org/packages/candidates/upload">candidate uploader</a> and give it a go.</p>
<h3 id="step-8---release-it">Step 8 - release it</h3>
<p>Some people like to leave their packages as candidates for a while to find bugs etc. The quality on hackage is generally pretty high so you want to avoid throwing things up there half-baked. Generally if you’ve followed the previous steps you should be in pretty good shape to release your package for real though.</p>
<pre class="sourceCode bash"><code class="sourceCode bash">$ <span class="kw">cabal</span> upload dist/your-pkg-x.y.z.tar.gz</code></pre>
<p>All done, right? Time to celebrate! Not necessarily. Recently Hackage has been failing to run haddock remotely to generate documentation. The maintainers claim there is a small delay but I have found sometimes it never happens. Thankfully <a href="https://twitter.com/kmett">Edward Kmett</a> created a script to build your own docs and push them to Hackage.</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="co">#!/bin/bash</span>
<span class="kw">set</span> <span class="kw">-e</span>

<span class="kw">if [</span> <span class="st">&quot;</span><span class="ot">$#</span><span class="st">&quot;</span> <span class="ot">-ne</span> 1<span class="kw"> ]</span>; <span class="kw">then</span>
  <span class="kw">echo</span> <span class="st">&quot;Usage: scripts/hackage-docs.sh HACKAGE_USER&quot;</span>
  <span class="kw">exit</span> 1
<span class="kw">fi</span>

<span class="ot">user=$1</span>

<span class="ot">cabal_file=$(</span><span class="kw">find</span> . -maxdepth 1 -name <span class="st">&quot;*.cabal&quot;</span> -print -quit<span class="ot">)</span>
<span class="kw">if [</span> <span class="ot">!</span> <span class="ot">-f</span> <span class="st">&quot;</span><span class="ot">$cabal_file</span><span class="st">&quot;</span><span class="kw"> ]</span>; <span class="kw">then</span>
  <span class="kw">echo</span> <span class="st">&quot;Run this script in the top-level package directory&quot;</span>
  <span class="kw">exit</span> 1
<span class="kw">fi</span>

<span class="ot">pkg=$(</span><span class="kw">awk</span> -F <span class="st">&quot;:[[:space:]]*&quot;</span> <span class="st">&#39;tolower($1)==&quot;name&quot;    { print $2 }&#39;</span> <span class="kw">&lt;</span> <span class="st">&quot;</span><span class="ot">$cabal_file</span><span class="st">&quot;</span><span class="ot">)</span>
<span class="ot">ver=$(</span><span class="kw">awk</span> -F <span class="st">&quot;:[[:space:]]*&quot;</span> <span class="st">&#39;tolower($1)==&quot;version&quot; { print $2 }&#39;</span> <span class="kw">&lt;</span> <span class="st">&quot;</span><span class="ot">$cabal_file</span><span class="st">&quot;</span><span class="ot">)</span>

<span class="kw">if [</span> <span class="ot">-z</span> <span class="st">&quot;</span><span class="ot">$pkg</span><span class="st">&quot;</span><span class="kw"> ]</span>; <span class="kw">then</span>
  <span class="kw">echo</span> <span class="st">&quot;Unable to determine package name&quot;</span>
  <span class="kw">exit</span> 1
<span class="kw">fi</span>

<span class="kw">if [</span> <span class="ot">-z</span> <span class="st">&quot;</span><span class="ot">$ver</span><span class="st">&quot;</span><span class="kw"> ]</span>; <span class="kw">then</span>
  <span class="kw">echo</span> <span class="st">&quot;Unable to determine package version&quot;</span>
  <span class="kw">exit</span> 1
<span class="kw">fi</span>

<span class="kw">echo</span> <span class="st">&quot;Detected package: </span><span class="ot">$pkg</span><span class="st">-</span><span class="ot">$ver</span><span class="st">&quot;</span>

<span class="ot">dir=$(</span><span class="kw">mktemp</span> -d build-docs.XXXXXX<span class="ot">)</span>
<span class="kw">trap</span> <span class="st">&#39;rm -r &quot;$dir&quot;&#39;</span> EXIT

<span class="kw">cabal</span> haddock --hoogle --hyperlink-source --html-location=<span class="st">&#39;/package/$pkg-$version/docs&#39;</span> --contents-location=<span class="st">&#39;/package/$pkg-$version&#39;</span>

<span class="kw">cp</span> -R dist/doc/html/<span class="ot">$pkg</span>/ <span class="ot">$dir</span>/<span class="ot">$pkg</span>-<span class="ot">$ver</span>-docs

<span class="kw">tar</span> cvz -C <span class="ot">$dir</span> --format=ustar -f <span class="ot">$dir</span>/<span class="ot">$pkg</span>-<span class="ot">$ver</span>-docs.tar.gz <span class="ot">$pkg</span>-<span class="ot">$ver</span>-docs

<span class="kw">curl</span> -X PUT \
     <span class="kw">-H</span> <span class="st">&#39;Content-Type: application/x-tar&#39;</span> \
     <span class="kw">-H</span> <span class="st">&#39;Content-Encoding: gzip&#39;</span> \
     <span class="kw">-u</span> <span class="st">&quot;</span><span class="ot">$user</span><span class="st">&quot;</span> \
     <span class="kw">--data-binary</span> <span class="st">&quot;@</span><span class="ot">$dir</span><span class="st">/</span><span class="ot">$pkg</span><span class="st">-</span><span class="ot">$ver</span><span class="st">-docs.tar.gz&quot;</span> \
     <span class="st">&quot;https://hackage.haskell.org/package/</span><span class="ot">$pkg</span><span class="st">-</span><span class="ot">$ver</span><span class="st">/docs&quot;</span></code></pre>
<p>Now you will have published a package that looks good and will be be easy for people to use. So get that code off your computer and onto Hackage and contribute to the glorious Haskell ecosystem!</p>
</div>

<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'begriffs';

  (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
]]></summary>
</entry>
<entry>
    <title>Writing controller specs for a Warp server</title>
    <link href="http://begriffs.com/posts/2014-10-19-warp-server-controller-test.html" />
    <id>http://begriffs.com/posts/2014-10-19-warp-server-controller-test.html</id>
    <published>2014-10-19T00:00:00Z</published>
    <updated>2014-10-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Writing controller specs for a Warp server</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">October 19, 2014</h5>
</div>

<div class="content">
  <p>This guide will show you how to configure a Cabal project to test a Warp server (such as Scotty or Yesod) using hspec2. It will allow you to test HTTP requests and responses and to prepare the database before/between steps.</p>
<h3 id="directories-and-cabal">directories and cabal</h3>
<p>Assuming your project code lives in a top-level <code>src</code> directory, create another top-level directory called <code>test</code>. We need to add a new section to the project cabal file that builds the test suite and can access the rest of the project. Append this and adjust package versions to taste:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="dt">Test</span><span class="fu">-</span><span class="dt">Suite</span> spec
  <span class="dt">Type</span><span class="fu">:</span>                exitcode<span class="fu">-</span>stdio<span class="fu">-</span><span class="fl">1.0</span>
  <span class="dt">Default</span><span class="fu">-</span><span class="dt">Language</span><span class="fu">:</span>    <span class="dt">Haskell2010</span>
  <span class="dt">Hs</span><span class="fu">-</span><span class="dt">Source</span><span class="fu">-</span><span class="dt">Dirs</span><span class="fu">:</span>      test, src
  ghc<span class="fu">-</span>options<span class="fu">:</span>         <span class="fu">-</span><span class="dt">Wall</span> <span class="fu">-</span><span class="dt">W</span> <span class="fu">-</span><span class="dt">Werror</span>
  <span class="dt">Main</span><span class="fu">-</span><span class="dt">Is</span><span class="fu">:</span>             Main.hs
  <span class="dt">Other</span><span class="fu">-</span><span class="dt">Modules</span><span class="fu">:</span>       <span class="co">-- other project modules</span>
  <span class="dt">Build</span><span class="fu">-</span><span class="dt">Depends</span><span class="fu">:</span>       base
                     , hspec2
                     , hspec<span class="fu">-</span>wai
                     , hspec<span class="fu">-</span>wai<span class="fu">-</span>json
                     , warp
                     , wai</code></pre>
<p>Add <code>test/Main.hs</code>. This is a good place to run other pre-test tasks like loading a database fixture.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">module</span> <span class="dt">Main</span> <span class="kw">where</span>

<span class="kw">import </span><span class="dt">Test.Hspec</span>
<span class="kw">import </span><span class="dt">Spec</span>

<span class="ot">main ::</span> <span class="dt">IO</span> ()
main <span class="fu">=</span> hspec spec</code></pre>
<p>Create <code>test/Spec.hs</code> containing this gobbledygook</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">{-# OPTIONS_GHC -F -pgmF hspec-discover -optF --no-main #-}</span></code></pre>
<p>This will search for and run all Haskell files <code>spec/**/*Spec.hs</code> as part of the test suite so you do not have to later remember to add new spec files to a configuration list. We’ll see later there is a way to focus the tests when running the suite.</p>
<p>Finally build your project and make it available for testing.</p>
<pre class="sh"><code>$ cabal install -j --enable-tests</code></pre>
<h3 id="beyond-simple-request-specs">beyond simple request specs</h3>
<p>The <a href="https://hackage.haskell.org/package/hspec-wai">hspec-wai</a> package contains some matchers to make things easy. Here’s an example from the readme showing simple reqeusts.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">app ::</span> <span class="dt">IO</span> <span class="dt">Application</span>
app <span class="fu">=</span> S.scottyApp <span class="fu">$</span> <span class="kw">do</span>
  S.get <span class="st">&quot;/&quot;</span> <span class="fu">$</span> <span class="kw">do</span>
    S.text <span class="st">&quot;hello&quot;</span>

<span class="ot">spec ::</span> <span class="dt">Spec</span>
spec <span class="fu">=</span> with app <span class="fu">$</span>
  describe <span class="st">&quot;GET /&quot;</span> <span class="fu">$</span> <span class="kw">do</span>
    it <span class="st">&quot;responds with 200&quot;</span> <span class="fu">$</span>
      get <span class="st">&quot;/&quot;</span> <span class="ot">`shouldRespondWith`</span> <span class="dv">200</span>

    it <span class="st">&quot;responds with &#39;hello&#39;&quot;</span> <span class="fu">$</span>
      get <span class="st">&quot;/&quot;</span> <span class="ot">`shouldRespondWith`</span> <span class="st">&quot;hello&quot;</span>

    it <span class="st">&quot;responds with 200 / &#39;hello&#39;&quot;</span> <span class="fu">$</span>
      get <span class="st">&quot;/&quot;</span> <span class="ot">`shouldRespondWith`</span> <span class="st">&quot;hello&quot;</span> {matchStatus <span class="fu">=</span> <span class="dv">200</span>}</code></pre>
<p>I like to run the tests with colored output and showing all steps, even those that pass.</p>
<pre class="sh"><code>$ cabal test -j --show-details=always --test-options=&quot;--color&quot;</code></pre>
<p>This is nice and simple, but what if you want to run a customized check on a response? For instance, let’s test that a header value matches a regex. You can do it inside lifted IO.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">{-# LANGUAGE OverloadedStrings #-}</span>
<span class="kw">import </span><span class="dt">Test.Hspec</span>
<span class="kw">import </span><span class="dt">Test.Hspec.Wai</span>
<span class="kw">import </span><span class="dt">Network.HTTP.Types</span>
<span class="kw">import </span><span class="dt">Network.Wai.Test</span> (<span class="dt">SResponse</span>(simpleHeaders,simpleStatus))
<span class="kw">import qualified</span> <span class="dt">Data.ByteString.Char8</span> <span class="kw">as</span> <span class="dt">BS</span>
<span class="kw">import </span><span class="dt">Data.CaseInsensitive</span> (<span class="dt">CI</span>(..))
<span class="kw">import </span><span class="dt">Text.Regex.TDFA</span> ((=~))

<span class="ot">matchHeader ::</span> <span class="dt">CI</span> <span class="dt">BS.ByteString</span> <span class="ot">-&gt;</span> <span class="dt">String</span> <span class="ot">-&gt;</span> [<span class="dt">Header</span>] <span class="ot">-&gt;</span> <span class="dt">Bool</span>
matchHeader name valRegex headers <span class="fu">=</span>
  maybe <span class="dt">False</span> (<span class="fu">=~</span> valRegex) <span class="fu">$</span> lookup name headers

<span class="ot">spec ::</span> <span class="dt">Spec</span>
spec <span class="fu">=</span> with app <span class="fu">$</span>
  describe <span class="st">&quot;GET /list&quot;</span> <span class="fu">$</span>
    it <span class="st">&quot;responds with valid range headers&quot;</span> <span class="fu">$</span> <span class="kw">do</span>
      r <span class="ot">&lt;-</span> request methodGet <span class="st">&quot;/list&quot;</span>
             [<span class="st">&quot;Range-Unit&quot;</span> <span class="fu">&lt;:&gt;</span> <span class="st">&quot;items&quot;</span>, <span class="st">&quot;Range&quot;</span> <span class="fu">&lt;:&gt;</span> <span class="st">&quot;0-9&quot;</span>]

      liftIO <span class="fu">$</span> <span class="kw">do</span>
        simpleHeaders r <span class="ot">`shouldSatisfy`</span>
          matchHeader <span class="st">&quot;Content-Range&quot;</span> <span class="st">&quot;^0-[0-9]+/[0-9]+$&quot;</span>
        simpleStatus r <span class="ot">`shouldBe`</span> partialContent206</code></pre>
<p>How do the tests know where to send requests? The trick is at the beginning in the <code>with</code> function which is an alias for <code>before</code> and defined in <code>hspec-wai</code> rather than <code>hspec</code> itself.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">before ::</span> <span class="dt">IO</span> a <span class="ot">-&gt;</span> <span class="dt">SpecWith</span> a <span class="ot">-&gt;</span> <span class="dt">Spec</span></code></pre>
<p>We give it an IO action, in this case <code>app :: IO Application</code> and it builds <code>SpecWith Application</code> which is a Reader monad that future test steps can query. For instance, the <code>request</code> method grabs the application internally with <code>getApp</code>.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- | Perform a request to the application under test, with specified HTTP</span>
<span class="co">-- method, request path, headers and body.</span>
<span class="ot">request ::</span> <span class="dt">Method</span> <span class="ot">-&gt;</span> <span class="dt">ByteString</span> <span class="ot">-&gt;</span> [<span class="dt">Header</span>] <span class="ot">-&gt;</span> <span class="dt">LB.ByteString</span>
                  <span class="ot">-&gt;</span> <span class="dt">WaiSession</span> <span class="dt">SResponse</span>
request method path headers body <span class="fu">=</span>
  getApp <span class="fu">&gt;&gt;=</span> liftIO <span class="fu">.</span> runSession (Wai.srequest <span class="fu">$</span> <span class="dt">SRequest</span> req body)
  <span class="kw">where</span>
    req <span class="fu">=</span> setPath defaultRequest
            {requestMethod <span class="fu">=</span> method, requestHeaders <span class="fu">=</span> headers} path</code></pre>
<p>Really <code>getApp</code> is a glorified <code>ask</code>, along with type constraints that would make it fail to compile if the surrounding test was not using <code>SpecWith Application</code>.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">getApp ::</span> <span class="dt">WaiSession</span> <span class="dt">Application</span>
getApp <span class="fu">=</span> <span class="dt">WaiSession</span> ask</code></pre>
<p>Another less explicitly monadic way to interact with the test subject is using an argument in the <code>it</code> function. What goes into <code>with</code> can come out in <code>it</code>.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">spec ::</span> <span class="dt">Spec</span>
spec <span class="fu">=</span> with (return <span class="dv">42</span>)<span class="ot"> ::</span> <span class="dt">IO</span> <span class="dt">Int</span> <span class="fu">$</span>
  describe <span class="st">&quot;This magical number&quot;</span> <span class="fu">$</span>
    it <span class="st">&quot;is bigger than 40&quot;</span> <span class="fu">$</span> \n <span class="ot">-&gt;</span>
      n <span class="ot">`shouldSatisfy`</span> (<span class="fu">&gt;</span><span class="dv">40</span>)</code></pre>
<p>Perhaps it’s silly as stated, but imagine the integer is a database connection instead. In fact this leads to the next topic…</p>
<h3 id="cleaning-the-db-between-tests">cleaning the db between tests</h3>
<p>In contrast to the <code>before</code> family of functions which use <code>SpecWith a</code> types, the <code>after</code> and <code>around</code> functions use <code>ActionWith a</code>. Internally it’s not much to speak of, but the alias will make our actions’ types read cleaner.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">type</span> <span class="dt">ActionWith</span> a <span class="fu">=</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</code></pre>
<p>What’s a good use case for around actions? Cleaning up the environment so tests do not pollute each other’s state is one. Here’s an example of rolling back any changes to a Postgres database after each test.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">import </span><span class="dt">Test.Hspec</span>
<span class="kw">import </span><span class="dt">Database.HDBC</span>
<span class="kw">import </span><span class="dt">Database.HDBC.PostgreSQL</span>
<span class="kw">import </span><span class="dt">Control.Exception.Base</span> (bracket)

<span class="ot">withDatabaseConnection ::</span> <span class="dt">ActionWith</span> <span class="dt">Connection</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> ()
withDatabaseConnection <span class="fu">=</span> bracket openConnection disconnect
  <span class="kw">where</span> openConnection <span class="fu">=</span> connectPostgreSQL&#39; <span class="st">&quot;postgres://etcetc&quot;</span>

<span class="ot">withRollback ::</span> <span class="dt">ActionWith</span> <span class="dt">Connection</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> ()
withRollback action <span class="fu">=</span> withDatabaseConnection <span class="fu">$</span> \c <span class="ot">-&gt;</span> <span class="kw">do</span>
  runRaw c <span class="st">&quot;begin;&quot;</span>
  action c
  rollback c

<span class="ot">spec ::</span> <span class="dt">Spec</span>
spec <span class="fu">=</span> around withRollback <span class="fu">$</span>
  describe <span class="st">&quot;inserting with abandon&quot;</span> <span class="fu">$</span>
    it <span class="st">&quot;does all kinds of things&quot;</span> <span class="fu">$</span> \conn <span class="ot">-&gt;</span>
      <span class="co">-- here we can use the connection and be assured</span>
      <span class="co">-- our sql commands will be rolled back</span></code></pre>
<p>One tiny but important detail is the choice of <code>connectPostgreSQL'</code> (with an apostrophe). The non-prime connect function in HDBC.PostgreSQL enables auto-commit. This means it peforms every statement in a transaction, which will cause surprises for you. The prime version is for manual transaction management like we are using here.</p>
<p>Simply combine the <code>with app</code> and <code>around withRollback</code> to do controller tests that include database cleaning. And don’t forget you can perform an action before the entire suite runs inside the <code>Main.hs</code> we created.</p>
</div>

<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'begriffs';

  (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
]]></summary>
</entry>

</feed>
