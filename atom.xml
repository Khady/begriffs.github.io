<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Begriffs.com blog</title>
    <link href="http://begriffs.com/atom.xml" rel="self" />
    <link href="http://begriffs.com" />
    <id>http://begriffs.com/atom.xml</id>
    <author>
        <name>Joe Nelson</name>
        <email>cred+blog@begriffs.com</email>
    </author>
    <updated>2015-04-05T00:00:00Z</updated>
    <entry>
    <title>Circular Statistics of SF Bus Routes</title>
    <link href="http://begriffs.com/posts/2015-04-05-sf-bus-routes-circular-statistics.html" />
    <id>http://begriffs.com/posts/2015-04-05-sf-bus-routes-circular-statistics.html</id>
    <published>2015-04-05T00:00:00Z</published>
    <updated>2015-04-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Circular Statistics of SF Bus Routes</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">April  5, 2015</h5>
</div>

<div class="content">
  <p><a href="http://erictheise.com/">Eric Theise</a>, Senior Software Engineer at <a href="http://repairpal.com/">RepairPal</a> asks, “Which way is Inbound?” Though SF Muni routes are prefaced with the modifiers “Inbound” and “Outbound”, their use of the terms often has little to do with the commonly understood meaning. In this presentation Eric uses open data, an open source geostack, and R’s circular package to visually and statistically analyze and discuss the resulting cognitive dissonance.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/124132910.hd.mp4?s=f047700972c48aaaf9c1e0c47a848240"
         poster="https://i.vimeocdn.com/video/513748329.png?mw=700"
  ></video>
</div>
<h3 id="summary">Summary</h3>
<ul>
<li>Overview of the San Francisco Muni - good coverage, bad timing</li>
<li>All routes are couched in terms of “inbound” and “outbound” which can be counterintuitive in curvy areas of some routes</li>
<li>The origin of the term “inbound” in navigation, and some San Francisco history</li>
<li>Quantifying the notion of “inbound”
<ul>
<li>we’ll attack the problem with statistical tools</li>
<li>mean direction, circular variance, and hypothesis testing</li>
</ul></li>
<li>Circular statistics avoids ambiguity in histograms that “wrap-around” where they can appear either bimodal or unimodal depending on where the wrapping point occurs</li>
<li>Two packages in R for this: circular and CircStates
<ul>
<li>Eric does not know of any python or javascript packages to do this kind of thing</li>
<li>He is working on <a href="Sr%20Engineer,%20RepairPal">mctad.js</a> to address this</li>
</ul></li>
<li>A great resource to learn more is the book <a href="http://circstatinr.st-andrews.ac.uk/">Circular Statistics in R</a></li>
<li>To analyze the concept of inbound and outbound we need route information
<ul>
<li>Eric got the routes from the nextbus API</li>
<li>Stored it in PostgreSQL with the PostGIS extension</li>
</ul></li>
<li>What is the true geospatial centroid of downtown?
<ul>
<li>Turns out it’s the Mechanic’s Monument, created by “the Michelangelo of the West”</li>
</ul></li>
<li>Demo
<ul>
<li>Filter routes, and examine each stop</li>
<li>Find mean direction and variance, stop by stop</li>
<li>Use hypothesis testing to see if in general the bus route has a straightforward labeling of inbound and outbound</li>
<li>Outbound is less defined than inbound as a place (it’s everywhere NOT inbound)</li>
</ul></li>
<li>For more info about Bay Area pedestrian and transit history see the maps of <a href="https://twitter.com/enf">Eric Fisher</a></li>
</ul>
</div>
]]></summary>
</entry>
<entry>
    <title>Tracking Joy at Work</title>
    <link href="http://begriffs.com/posts/2015-03-15-tracking-joy-at-work.html" />
    <id>http://begriffs.com/posts/2015-03-15-tracking-joy-at-work.html</id>
    <published>2015-03-15T00:00:00Z</published>
    <updated>2015-03-15T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Tracking Joy at Work</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">March 15, 2015</h5>
</div>

<div class="content">
  <p>You can’t improve what you can’t measure, and what better to measure than an activity that consumes your waking life. Work.</p>
<p>Work, that oft-maligned but secretly important part of having meaning in life. Why are some days frustrating and others swept in effortless flow? Are bad days shared among an entire office or suffered individually?</p>
<p>I’ve created a program to help answer this question. Throughout the day at random intervals it privately sends me and my coworkers at <a href="http://looprecur.com">Loop/Recur</a> messages on Slack to see how we’re feeling. We respond on a scale of one to five and the results are displayed anonymously on a realtime dashboard.</p>
<div class="figure">
<img src="/images/happiness-dashboard.png" alt="happiness dashboard" /><p class="caption">happiness dashboard</p>
</div>
<p>Just asking this basic question reveals a lot and has other good side effects. Its simplest effect is to make us reflect on our mood. Quantifying how we feel raises us above inarticulate reaction. Also we can spot bad moods that we all share because they appear on the dashboard. It sparks helpful discussions.</p>
<p>The technique of sampling moods or other subjective reports over time is called the Experience Sampling Method and has been done since the 1970s. It’s been getting easier to conduct this research as communication and data collection become more accessible to computers. Not so long ago pagers were the best way to alert people to respond.</p>
<p>While developing the program I consulted a fascinating book called, “Experience Sampling Method: Measuring the Quality of Everyday Life.” I wanted to be acquainted with the methodology behind these kind of experiments. Whether or not you’re interested in data science I’d recommend you check this book out. It’s full of studies that talk about things that are fundamental to human happiness, from family life to work to gender differences. <img src="/images/experience-sampling.jpeg" style="float:right" /></p>
<p>The three common types of sampling are event contingent, interval contingent, and signal contingent. The first is where the participant makes reports during a certain type of event. At our software company it might be after committing code to git, or coming back from lunch. The second is when the participant makes recordings at the end of large intervals, like at the end of the day. We chose the last method, signal contingency.</p>
<p>Signal contingent sampling is where participants get random prompts to record data. It helps avoid memory biases and measures all computer-based activities without preference.</p>
<p>To run the system we have a daily scheduler. It picks three random times from a uniform distribution inside each user’s time preference window (the hours during which we each prefer to be solicited.)</p>
<p>The randomness of the sampling is interesting. Sometimes I may get solicited twice within a few minutes, other times more evenly throughout the day. We have considered enforcing a more even solicitation schedule (making it impossible that they occur too close together) but we don’t have a clear justification for the change. For the time being we’re leaving it as a uniform, sometimes lumpy, distribution.</p>
<p>We don’t yet have enough data to make statistically significant claims about how we feel, but it has sure been fun keeping an eye on the dashboard. Many companies have big screens showing web analytics, but we’re the first I’ve seen to track how we feel – and it feels good.</p>
<div class="alert alert-info" role="alert">
<h4>
Russian Translation
</h4>
Пост доступен на сайте softdroid.net: <a
href="http://softdroid.net/otslezhivanie-radosti-na-rabote">Softdroid: исследование производительности на работе</a>.
</div>
</div>
]]></summary>
</entry>
<entry>
    <title>Machine Learning at the Limit</title>
    <link href="http://begriffs.com/posts/2015-03-13-machine-learning-at-limit.html" />
    <id>http://begriffs.com/posts/2015-03-13-machine-learning-at-limit.html</id>
    <published>2015-03-13T00:00:00Z</published>
    <updated>2015-03-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Machine Learning at the Limit</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">March 13, 2015</h5>
</div>

<div class="content">
  <p>Professor <a href="http://www.cs.berkeley.edu/~jfc/">John Canny</a> presents benchmarks for the <a href="https://github.com/BIDData/BIDMach">BIDMach</a> Scala machine learning library. He and his team achieve a nearly theoretical maximum speed in this library relative to roofline analysis of the available CPU and GPU resources. They have managed to outperform entire machine learning clusters using a single computer. Check out the video below for more detailed comparisons.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/122075036.hd.mp4?s=c560edc9e62183abe4542a1f8b7a9bd2"
         poster="https://i.vimeocdn.com/video/510970022.png?mw=700"
  ></video>
</div>
<p>This talk was delivered at <a href="http://alpinenow.com/">Alpine Data Lab</a>.</p>
<a class="embedly-card" href="http://www.slideshare.net/ChesterChen/sf-big-analytics">Slides</a>
<script async src="//cdn.embedly.com/widgets/platform.js" charset="UTF-8"></script>
<h3 id="summary">Summary</h3>
<ul>
<li>Discovering the limits of machine learning performance using roofline design from computer architecture</li>
<li>Taking fully accelerated single-node algorithms and scaling them up</li>
<li>Tweaking a data model for machine learning should be quick and iterative</li>
<li>Often prototype machine learning is done in Scikit-Learn or R. It is then translated for production code by a group that often lacks machine learning expertise, which degrades the quality of results.</li>
<li>The reasons for developing a new ML toolkit (Bidmach):
<ul>
<li>GPUS now operate efficiently on sparse data</li>
<li>minibatch and stochastic gradient descent habe become a workhorse of choice for big data processing</li>
<li>easy customization of models</li>
<li>quick exploration, and sharing code between prototype and production</li>
</ul></li>
<li>To design the framework we start with roofline design and articulating the limits of CPUs etc</li>
<li>We build the high level operations on top of a GPU-accelerated matrix library</li>
<li>Roofline design establishes fundamental performance limits for a computational kernel</li>
<li>Comparison of the strengths of CPUs and GPUs</li>
<li>Comparing performance of BIDMach vs Vowpal Wabbit vs Scikit-Learn</li>
<li>For many tasks a single node running BIDMach outperforms a whole cluster running traditional libraries</li>
<li>Measurements of running time for Latent Dirichlet Allocation, random forests, single-class regression, logistic regression, multilabel regression, factor models, SVMs and clustering</li>
<li>Rooflining network bandwidth</li>
</ul>
</div>
]]></summary>
</entry>
<entry>
    <title>Better Tweets Through Data Science</title>
    <link href="http://begriffs.com/posts/2015-03-10-better-tweets-datascience.html" />
    <id>http://begriffs.com/posts/2015-03-10-better-tweets-datascience.html</id>
    <published>2015-03-10T00:00:00Z</published>
    <updated>2015-03-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Better Tweets Through Data Science</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">March 10, 2015</h5>
</div>

<div class="content">
  <div class="figure">
<img src="/images/bird-word.png" alt="bird words" /><p class="caption">bird words</p>
</div>
<p>Great tweets aren’t born; they’re made. It turns out there are practical steps anyone can take to give their tweets a boost. I’m going to share lessons I learned applying data science to Twitter to discover what sets the top performing tweets apart from the un-favorited masses. This article is basically a cookbook you can use to construct some tasty tweets no matter what their topic.</p>
<p>First things first. To have any chance at creating top tweets you need to start with interesting and useful content. Sorry, no amount of code or statistics can make up for a fundamentally boring article.</p>
<p>Let’s begin by assuming you have created or discovered something great to share. It is probably interesting for quite a few reasons. The trick is to recognize <em>which</em> reason most interests other people. If you emphasize the unpopular aspects of content it will appear that nobody is listening. In fact the first step is to do some listening yourself.</p>
<p>You certainly have some intuitions of what interests others, but we can go beyond fuzzy intuition and tap into that huge record that is the internet.</p>
<div class="alert alert-info" role="alert">
<h4>
Observation One
</h4>
What interests a large group of people today is what interested them yesterday.
</div>
<p>It’s our job to take the record of interest – the favorites, the retweets – and distill their topics. Not only the literal subjects under consideration but the psychological needs of the audience. This becomes more straightforward using the right algorithms.</p>
<h3 id="finding-topics">Finding Topics</h3>
<p>The first and most blunt tool for exploring topics is obviously the hashtag. They’re great both because people use them to self-identify topics and because there are existing tools online to judge hashtag popularity and relatedness.</p>
<p>Let’s get started, try this along with me. Pick a broad (unspecific) hashtag related to what you want to share. I’ll pick the super generic tag #data. We want to find other tags that are fairly popular and describe our content more precisely. To explore this use hashtagify.me</p>
<div class="figure">
<img src="/images/hashtag-graph.png" alt="Related hashtags" /><p class="caption">Related hashtags</p>
</div>
<p>Sometimes you’ll find that a tag isn’t used the way you expected. For instance I found that #cluster has more to do with jewelry than it does with cloud computing.</p>
<p>The #data graph above shows that we might want to investigate #BigData, or #privacy instead. But for this example let’s stick with #data. To really get to know the personality of this hashtag we’ll need data. So visit https://twitter.com/search-home and try #data (or your own choice of tag).</p>
<p>You’ll notice there is “Top” or “All” mode. We want to stay within Top because we want to learn about winners, not the others. Now we could use the Twitter API to programmatically download a bunch of data but we would have to sign up for access keys etc.</p>
<p>Let’s just do it the quick and easy way for now. Start scrolling down the search page causing new results to be loaded. Just keep scrolling for a long while until you have many pages of info.</p>
<p>Now use a quick snippet of JavaScript to grab the tweet text for processing. Pop open the JavaScript console in your browser (Cmd-Option-J in Chrome). Paste this in and press Enter:</p>
<pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="fu">$</span>(<span class="st">&#39;.tweet-text&#39;</span>).<span class="fu">map</span>(<span class="kw">function</span> () {
  <span class="kw">return</span> <span class="ot">$</span>.<span class="fu">trim</span>(<span class="fu">$</span>(<span class="kw">this</span>).<span class="fu">text</span>());
}).<span class="fu">get</span>().<span class="fu">join</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</code></pre>
<h3 id="revealing-patterns">Revealing Patterns</h3>
<p>Select all the output and save it to a file. We’re going to extract meaning from this text. Remember that we want the essential truth contained inside so we need to remove words which would distract from or skew the essence. Save and run the following shell script on your file to remove urls, competing hash tags, and @-mentions.</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">perl</span> -p -i -e <span class="st">&quot;s/@</span><span class="dt">\\</span><span class="st">w+[ ,</span><span class="dt">\\</span><span class="st">.</span><span class="dt">\\</span><span class="st">?</span><span class="dt">\&quot;</span><span class="st">:&#39;]//g&quot;</span> <span class="ot">$1</span>
<span class="kw">perl</span> -p -i -e <span class="st">&#39;s/@\w+$//g&#39;</span> <span class="ot">$1</span>

<span class="kw">perl</span> -p -i -e <span class="st">&#39;s/#\w+[ ,\.\?&quot;:]//g&#39;</span> <span class="ot">$1</span>
<span class="kw">perl</span> -p -i -e <span class="st">&#39;s/#\w+$//g&#39;</span> <span class="ot">$1</span>

<span class="kw">perl</span> -p -i -e <span class="st">&#39;s/pic.twitter[^ ]+ //g&#39;</span> <span class="ot">$1</span>
<span class="kw">perl</span> -p -i -e <span class="st">&#39;s/pic.twitter[^ ]+$//g&#39;</span> <span class="ot">$1</span>

<span class="kw">perl</span> -p -i -e <span class="st">&#39;s/https?:[^ ]+ //g&#39;</span> <span class="ot">$1</span>
<span class="kw">perl</span> -p -i -e <span class="st">&#39;s/https?:[^ ]+$//g&#39;</span> <span class="ot">$1</span></code></pre>
<p>We’ll use the R programming language to run topic detection through Latent Dirichlet Allocation (LDA). Here are the pieces of code to calculate them.</p>
<p>First load the tweets, one on each line. They will each be considered small “documents” for topic detection, and are put together into a “corpus.”</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(jsonlite)
<span class="kw">library</span>(tm)

tweets &lt;-<span class="st"> </span><span class="kw">strsplit</span>(<span class="kw">readLines</span>(<span class="st">&#39;/path/to/tweets.txt&#39;</span>), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)
corp &lt;-<span class="st"> </span><span class="kw">Corpus</span>(<span class="kw">VectorSource</span>(tweets))</code></pre>
<p>To minimize inconsequential variations we will remove stop words, small words and other detritus. The remaining words are stemmed.</p>
<pre class="sourceCode r"><code class="sourceCode r">dtm.control &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">tolower =</span> <span class="ot">TRUE</span>,
                    <span class="dt">removePunctuation =</span> <span class="ot">TRUE</span>,
                    <span class="dt">removeNumbers =</span> <span class="ot">TRUE</span>,
                    <span class="dt">stopwords =</span> <span class="kw">c</span>(<span class="kw">stopwords</span>(<span class="st">&quot;SMART&quot;</span>),
                                  <span class="kw">stopwords</span>(<span class="st">&quot;en&quot;</span>)),
                    <span class="dt">stemming =</span> <span class="ot">TRUE</span>,
                    <span class="dt">wordLengths =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="st">&quot;inf&quot;</span>),
                    <span class="dt">weighting =</span> weightTf)</code></pre>
<p>Next we record the term occurrences per document in a sparse matrix and set up parameters. This is a “bag-of-words” approach that ignores the grammar of sentences.</p>
<pre class="sourceCode r"><code class="sourceCode r">sparse.dtm &lt;-<span class="st"> </span><span class="kw">DocumentTermMatrix</span>(corp, <span class="dt">control =</span> dtm.control)
dtm &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(sparse.dtm)
<span class="kw">class</span>(dtm) &lt;-<span class="st"> &quot;integer&quot;</span>

vocab &lt;-<span class="st"> </span>sparse.dtm$dimnames$Terms
<span class="co"># Compute some statistics related to the data set:</span>
D &lt;-<span class="st"> </span><span class="kw">length</span>(corp)  <span class="co"># number of documents</span>
W &lt;-<span class="st"> </span><span class="kw">length</span>(vocab)  <span class="co"># number of terms in the vocab</span>
doc.length &lt;-<span class="st"> </span><span class="kw">rowSums</span>(dtm)  <span class="co"># number of tokens per document</span>
N &lt;-<span class="st"> </span><span class="kw">sum</span>(doc.length)  <span class="co"># total number of tokens in the data</span>
term.frequency &lt;-<span class="st"> </span><span class="kw">colSums</span>(dtm)  <span class="co"># frequencies of terms in the corpus</span>

<span class="co"># MCMC and model tuning parameters:</span>
K &lt;-<span class="st"> </span><span class="dv">20</span>
G &lt;-<span class="st"> </span><span class="dv">5000</span>
alpha &lt;-<span class="st"> </span><span class="fl">0.02</span>
eta &lt;-<span class="st"> </span><span class="fl">0.02</span></code></pre>
<p>Using the term frequencies we attempt to find clusters or words that tend to co-occur. This is the part that really identifies topics in tweets.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit the model:</span>
<span class="kw">library</span>(lda)

lda.input &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="dv">1</span>:<span class="kw">nrow</span>(dtm), function (i) {
    docfreq &lt;-<span class="st"> </span><span class="kw">t</span>(dtm[i,])
    keepers &lt;-<span class="st"> </span>docfreq &gt;<span class="st"> </span><span class="dv">0</span>
    <span class="kw">rbind</span>( (<span class="dv">0</span>:(<span class="kw">ncol</span>(dtm)-<span class="dv">1</span>))[keepers], <span class="kw">t</span>(dtm[i,])[keepers] )
  } )

fit &lt;-<span class="st"> </span><span class="kw">lda.collapsed.gibbs.sampler</span>(<span class="dt">documents =</span> lda.input,
                                   <span class="dt">K =</span> K, <span class="dt">vocab =</span> vocab,
                                   <span class="dt">num.iterations =</span> G, <span class="dt">alpha =</span> alpha,
                                   <span class="dt">eta =</span> eta, <span class="dt">initial =</span> <span class="ot">NULL</span>, <span class="dt">burnin =</span> <span class="dv">0</span>,
                                   <span class="dt">compute.log.likelihood =</span> <span class="ot">TRUE</span>)</code></pre>
<p>Finally plug the topic model into an interactive visualization that we will examine with our human intuition.</p>
<pre class="sourceCode r"><code class="sourceCode r">theta &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(fit$document_sums +<span class="st"> </span>alpha, <span class="dv">2</span>, function(x) x/<span class="kw">sum</span>(x)))
phi &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(<span class="kw">t</span>(fit$topics) +<span class="st"> </span>eta, <span class="dv">2</span>, function(x) x/<span class="kw">sum</span>(x)))

<span class="kw">library</span>(LDAvis)

<span class="co"># create the JSON object to feed the visualization:</span>
json &lt;-<span class="st"> </span><span class="kw">createJSON</span>(<span class="dt">phi =</span> phi,
                   <span class="dt">theta =</span> theta,
                   <span class="dt">doc.length =</span> doc.length,
                   <span class="dt">vocab =</span> vocab,
                   <span class="dt">term.frequency =</span> term.frequency)

<span class="kw">serVis</span>(json)</code></pre>
<p>Time for the fun. Here’s a screenshot of the topics detected in top tweets having the #data hashtag. I have selected topic number 19 to see the frequency of the words inside. Notice I set the relevance metric slider low to focus on words that are frequent exclusively in the topic rather than frequent in the corpus as a whole.</p>
<div class="figure">
<img src="/images/tweet-topics.png" alt="LDA topics" /><p class="caption">LDA topics</p>
</div>
<h3 id="extracting-meaning">Extracting Meaning</h3>
<p>The topics can be strange. If they are too chaotic and strange it probably means the data was not sufficiently cleaned or the sample was too small. But the more you look coherent categories the more you’ll see a kind of alien logic at work picking out words with a psychological bond.</p>
<p>At this point the algorithms have done their job and it is time for us to be creative as humans. Let me walk you through how I made sense of the topics for the #data hashtag.</p>
<p>For each detected topic I’ll list the main word, the feelings I associate with the topic, and the other major words inside that guided my assessment.</p>
<ul>
<li>The first (TRIAL) topic is cajoling people to try data products. It emphasizes the ease and quickness, and how the solution will “unlock” things, make you smarter, and amaze your boss.
<ul>
<li>storage, team, record, train, unlock, sport, free, clean, bottom, device, amazing, quick, boss, smarter</li>
</ul></li>
<li>Then we have the (CAPITAL) topic, all about disrupting industries and big company speculation.
<ul>
<li>capital, disrupt, venture, checkout, seeker, relationship, patent, airline, google, bet, fresh, double, fund</li>
</ul></li>
<li>The (BITCOIN) topic deals with that currency but also with non-monetary communication
<ul>
<li>communication, blah, exvers, search, speak, link, bitcoin, block, discount, listen, word, library, chain, song, teach, topic, print</li>
</ul></li>
<li>People worry about data, and the (BREACH) topic is full of credit cards, security compromises, and how much it costs us all.
<ul>
<li>credit, hit, compromise, driver, uber, interpret, human, card, replace, commission, hardware, neutral, pool, cost</li>
</ul></li>
<li>Of course you can’t have #data without (BIG). Apparently we’re “drowning” in it and it’s a veritable “lake,” but “wow” it is so big.
<ul>
<li>industry, lake, reach, love, move, drown, session, loyalty, wow</li>
</ul></li>
<li>The (OPPORTUNITY) topic deals with vision and red-hot engineers in the valley. You can not only wrangle data, you can be a data king.
<ul>
<li>location-based, senior, apply, hadoop, develop, succeed, hire, engineer, need, wrangle, vision, creator, gather, king, cluster, kingdom, hot, always, valley, red</li>
</ul></li>
<li>Then we have (BRANDS) and their associated trust words. People want to rate them and make decisions.
<ul>
<li>rate, trust, begin, smart, comfort, brain, decision, healthcare, focus, true, specific, rapid, crime, machine, vital, care, attribute, past, player, influence, patient</li>
</ul></li>
<li>Whereas opportunity was about creative struggle and power, (ADVANTAGE) is more about reassurance. A.k.a. debunking myths, providing answers, and strategizing.
<ul>
<li>advertis, myth, valuable, password, fuel, answer, data-driven, tip, like, competitive, autom, raw, break, provid, deliv, strategi, integr, analysi, format</li>
</ul></li>
<li>Beyond the technical aspects we have (LEGAL) implications. Laws passed or killed, policities about encryption and telecommunications.
<ul>
<li>phone, law, talktalk, combine, benefit, energi, save, stolen, plan, summari, encrypt, communicate, kill, pass, appeal, backup, transit, fall, bus, attack</li>
</ul></li>
</ul>
<div class="alert alert-info" role="alert">
<h4>
Observation Two
</h4>
Topics generated by LDA reveal strange, sometimes psychological connections in tweets.
</div>
<p>Is this subjective? Very much so. But I am searching for archetypal tweet structures, and some topics can be fairly conclusive. I have run this process on several hashtags and can definitely sense their distinct personalities.</p>
<p>For instance, check out the related tag #datascience. It should be #data’s close cousin but it does reveal that there is an industry of teaching data science. It is career-oriented.</p>
<ul>
<li>The (HIRE) shows people want to work in the field but are overwhelmed and turn to instructors.
<ul>
<li>recruit, rocket, role, survey, rocket, bank, overwhelm, instructor, begin, bet, billion</li>
</ul></li>
<li>More specific (RESOURCES)
<ul>
<li>bay, behavior, education, video, school, bootcamp, hot, write</li>
</ul></li>
<li>This topic, (MAKE), is similar to the OPPORTUNITY topic in #data but is less power-oriented. It tells a story of a journey to a beautiful outcome.
<ul>
<li>journey, digital, consider, transform, data-driven, easier, qualiti, visualis, metadata, simpli, measur, beauti, outcome, effect</li>
</ul></li>
<li>Welcome to the breathless (FUTURE) where things are emerging, things to watch
<ul>
<li>social, impact, emerging, tech, database, past, watch, attract, consum, answer, chicken&amp;egg, law</li>
</ul></li>
<li>Data science has well paying (COMPETITIONS)
<ul>
<li>learn, kaggle, spend, guest, call, competition, firm, persuade, architect, rule, half, competitor</li>
</ul></li>
<li>Sure these fancy science topics sound great, but show me a practical (APPLICATION).
<ul>
<li>develop, find, practical, succeed, economi, attend, webinar, cookbook</li>
</ul></li>
<li>Apparently (MACHINE LEARNING) is where the hard math lives
<ul>
<li>math, hard, google, library, word, oxford, advanced</li>
</ul></li>
<li>What is the biggest (STORY) of the week that you don’t want to miss?
<ul>
<li>week, news, bottleneck, back, biggest, tweet, reason, miss, tell, home, demand</li>
</ul></li>
<li>Statisticians explain (HISTORY) with infographics
<ul>
<li>integral, infographic, statistician, api, century, automate, count, sexiest</li>
</ul></li>
<li>Not just a (PATTERN) but a deep one, found faster, bigger and smarter. Boom.
<ul>
<li>program, visual, deep, language, beginn, random, linear, regress, faster, explore, bigger, smarter, storytell, connect, generate</li>
</ul></li>
<li>I call it the (ANALYTICS) topic but you might also call it the magic topic. Realtime things for experts. It tracks stuff and never stops.
<ul>
<li>realtime, message, queue, test, predict, track, influence, expert, review, embrace, magic</li>
</ul></li>
</ul>
<h3 id="crafting-the-tweets">Crafting the Tweets</h3>
<p>So we found the main psychological topics in top tweets from our categories. How can we construct new irresistably sharable tweets? Use the topic words as your guide. Let’s think about this article itself. We could consider it in the cringe-worthy genre of social media marketing techniques and learn more about those hashtags, but let’s think of it as #datascience and #data.</p>
<p>This article does deal with making things (tweets) with an anticipated outcome, so let’s cast it as a MAKE topic. Review the words in the topic again to see how I used them.</p>
<blockquote>
<p>My #datadriven journey to measure and transform my tweets</p>
</blockquote>
<p>It is also an APPLICATION of a particular algorithm, so dressing it in those clothes we get something like</p>
<blockquote>
<p>A cookbook for practical #datascience to make your tweets succeed</p>
</blockquote>
<p>But it provides OPPORTUNITIES to improve, hence</p>
<blockquote>
<p>How to create successful tweets by wrangling Twitter #data with #textmining</p>
</blockquote>
<p>Of course we’re dealing with PATTERNS and we can mimic that topic with something like,</p>
<blockquote>
<p>Exploring deep language patterns to tweet smarter (#datascience)</p>
</blockquote>
<h3 id="conclusion">Conclusion</h3>
<p>We’ve seen how to explore the space of hashtags, find topics in each one, and use our intuition to find psychological motives in topics. We then used the topic words as inspiration for how to communicate with others in the language that has proven most effective.</p>
<p>Certainly a bag-of-words approach such as we’ve outlined is limited, and leaning on it for every tweet will probably just sound generic. Ultimately Twitter is a place to have normal conversations with people. However I have found that algorithmically analyzing topics helps me see other people’s point of view and write more compelling content.</p>
</div>
]]></summary>
</entry>
<entry>
    <title>Text Mining in R (Sentiment Analysis, LDA, and Syuzhet)</title>
    <link href="http://begriffs.com/posts/2015-02-25-text-mining-in-r.html" />
    <id>http://begriffs.com/posts/2015-02-25-text-mining-in-r.html</id>
    <published>2015-02-25T00:00:00Z</published>
    <updated>2015-02-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Text Mining in R (Sentiment Analysis, LDA, and Syuzhet)</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">February 25, 2015</h5>
</div>

<div class="content">
  <p><a href="https://www.linkedin.com/pub/harold-baize/4/480/529">Harold Baize</a>, researcher at the <a href="https://www.sfdph.org/dph/default.asp">San Francisco Department of Public Health</a> shows how to use the latest <a href="http://www.r-project.org/">R</a> packages to analyze sentiments and topics in text. He gives a demo of using mental health provider notes to assess the effectiveness of treatments. Along the way he shows how disciplines from psychology to literary analysis have pioneered the practices labeled under the umbrella of “text mining.”</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/120614691.hd.mp4?s=d86271a555347fc538b2084083ef16bf"
         poster="https://i.vimeocdn.com/video/508549850.png?mw=700"
  ></video>
</div>
<h3 id="summary">Summary</h3>
<ul>
<li>Origins of “text mining” in the “content analysis” of psychology</li>
<li>We’ll be looking at sentiment analysis of content modeling</li>
<li>Both are bag-of-words approaches</li>
<li>Facebook’s quasi-unethical emotion manipulation experiment using sentiment analysis</li>
<li>Great R packages: tm, topicmodels, LDAvis, syzuhet</li>
<li>Definition of terms used</li>
<li>Doing some actual text mining
<ul>
<li>Begin with a document term matrix</li>
<li>Built using “controls” like stop words, stemming etc</li>
<li>Then merge the term matrix with the lexicon of emotional values</li>
<li>There are some common sentiment lexicons ready made: ANEW, Afinn</li>
</ul></li>
<li>Sentiment analysis can go beyond merely positive/negative
<ul>
<li>We can record the holder, target, sentiment, polarity, and source of a corpus</li>
</ul></li>
<li>Beware that bag-of-words approach misses things like sarcasm, double entendre and negation</li>
<li>A walkthrough of analyzing the sentiment of a psychological health review shows sometimes we need to modify an off the shelf lexicon.</li>
<li>Comparing sentiment in this study
<ul>
<li>Visualizing sentiment scores on psych progress notes to compare different hospitals</li>
<li>Different ethnicities show show differences in positivity</li>
<li>Also little kids/toddlers are way more optimistic, then get steadily more negative toward middle age. Finally above age 60 people tend to get positive again, at the level of 5-12 year olds.</li>
</ul></li>
<li>Predictions using sentiment data</li>
<li>Topic modeling
<ul>
<li>An unsupervised method to find latent topics</li>
<li>LDA (latent dirichlet allocation) is a popular form of topic modeling</li>
<li>It returns the probability that each word is found in each topic</li>
<li>We can visualize the results with LDAvis</li>
</ul></li>
<li>Example of doing LDA
<ul>
<li>Configure params like burn-in, iterations, number of topics, method of sampling and how frequently to keep samples</li>
<li>Sometimes the topics are difficult to interpret, and this can be exacerbated by poor data cleaning</li>
</ul></li>
<li>Demo of exploring the output of LDA
<ul>
<li>Adjusting relevance vs frequency in corpus or topic</li>
<li>The results are probabilistic, so each run produces different topics, but some of them persist</li>
</ul></li>
<li>Predicting topics
<ul>
<li>For instance a linear model reveals topics associated with newcomers to psychiatry vs old timers</li>
</ul></li>
<li>Concepts from literary analysis have been encoded into algorithms to help us understand a story
<ul>
<li>syuzhet vs fabula</li>
<li>A video that demonstrates the syuzhet of fictional story arcs</li>
</ul></li>
<li>Applying the syuzhet library in R
<ul>
<li>The Canadian National Research Council has created a rich measure of syuzhet along ten dimension of emotion, so you can watch e.g. disgust or anticipation change over time</li>
</ul></li>
<li>There is also a supervised version of LDA (SLDA) but it is not covered in this talk</li>
</ul>
</div>
]]></summary>
</entry>
<entry>
    <title>How Transparent Encryption Works in HDFS</title>
    <link href="http://begriffs.com/posts/2015-02-22-hdfs-encryption.html" />
    <id>http://begriffs.com/posts/2015-02-22-hdfs-encryption.html</id>
    <published>2015-02-22T00:00:00Z</published>
    <updated>2015-02-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>How Transparent Encryption Works in HDFS</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">February 22, 2015</h5>
</div>

<div class="content">
  <p><a href="https://www.linkedin.com/pub/charles-lamb/0/49a/694">Charles Lamb</a>, software enginner at <a href="http://www.cloudera.com/content/cloudera/en/home.html">Cloudera</a>, describes the tradeoffs between various levels of encryption, the choices he made when designing transparent encryption in HDFS, and the concepts you need to understand to use it.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/120274127.hd.mp4?s=3d80990f0d19452f2133316671a594f2"
         poster="https://i.vimeocdn.com/video/508061204.png?mw=700"
  ></video>
</div>
<h3 id="summary">Summary</h3>
<ul>
<li>Transparent encryption: data is read and written to an encrypted subtree on HDFS</li>
<li>This helps helps applications be regulation-compliant</li>
<li>Encryption/decryption is always handled by the client, HDFS itself never sees plaintext</li>
<li>The levels of encryption:
<ul>
<li>Application (hard to do and to add to legacy apps)</li>
<li>Database (prone to leaks through e.g. secondary indices)</li>
<li>Filesystem (higher performance, transparent, less flexible for various tenants)</li>
<li>Disk level (only protects against physical theft)</li>
</ul></li>
<li>HDFS transparent encryption lives somewhere between db and filesystem levels</li>
<li>Design goals</li>
<li>In-depth explanations of architectural concepts
<ul>
<li>Key-management server</li>
<li>Encryption zones</li>
<li>Keys</li>
</ul></li>
<li>HDFS encryption configuration</li>
<li>Per-user and per-key ACLs</li>
<li>Performance results</li>
</ul>
</div>
]]></summary>
</entry>
<entry>
    <title>Deploying Microservices</title>
    <link href="http://begriffs.com/posts/2015-02-15-microservice-template.html" />
    <id>http://begriffs.com/posts/2015-02-15-microservice-template.html</id>
    <published>2015-02-15T00:00:00Z</published>
    <updated>2015-02-15T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Deploying Microservices</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">February 15, 2015</h5>
</div>

<div class="content">
  <p>I explain my new open-source experiment, the <a href="https://github.com/begriffs/microservice-template">microservice-template</a> which you can use to deploy, scale, and monitor any kind of cloud microservices. The project contains preconfigured server definitions for common programs for collecting statistics, making a dashboard, and managing work queues.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/119660173.hd.mp4?s=c01b34d9882a7a179c3b885b8d523e0e"
         poster="https://i.vimeocdn.com/video/507185690.png?mw=700"
  ></video>
</div>
<h3 id="summary">Summary</h3>
<ul>
<li>Initial motivation: building a distributed scraping system</li>
<li>Need a scalable architecture for high bandwidth and to avoid rate-limiting</li>
<li>Ran into the problem of insufficient feedback about jobs</li>
<li>We need feedback to avoid accidental DDoS and for debugging the system</li>
<li>The first half of the solution is to log absolutely everything</li>
<li>The other half is system resource usage sampling</li>
<li>Then the logs must be consolidated and presented in a dashboard</li>
<li>Next big goal: simply to deploy, reproducible behavior, and flexible operation</li>
<li>The technologies inside and how they work together
<ul>
<li>Grafana</li>
<li>InfluxDB</li>
<li>Statsd</li>
<li>Collectd</li>
<li>RabbitMQ</li>
<li>Packer</li>
<li>Consul</li>
<li>Terraform</li>
<li>Chef</li>
</ul></li>
<li>Servers should be disposable commodities</li>
<li>They also need to be able to discover each other dynamically</li>
<li>The system turns out to be promising for services beyond just scrapers</li>
<li>The philosophy of rederiving data as needed from a master copy</li>
<li>Demo</li>
</ul>
</div>
]]></summary>
</entry>
<entry>
    <title>Filling Haskell's Type Holes Like It's Agda</title>
    <link href="http://begriffs.com/posts/2015-02-07-haskell-type-holes.html" />
    <id>http://begriffs.com/posts/2015-02-07-haskell-type-holes.html</id>
    <published>2015-02-07T00:00:00Z</published>
    <updated>2015-02-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Filling Haskell's Type Holes Like It's Agda</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">February  7, 2015</h5>
</div>

<div class="content">
  <p><a href="https://github.com/kirstin-rhys">Kirstin Rhys</a> gives a live coding demo of interactively building Haskell functions by filling type holes. This allows her to write abstract things which would be quite a challenge for the unaided mind. Interactive type holes are used in provers like Agda and Idris and are now supported by GHC 7.8.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/118981555.hd.mp4?s=6059f3a381a241aa0608a37fe6e9c095"
         poster="https://i.vimeocdn.com/video/506146633.png?mw=700"
  ></video>
</div>
<h3 id="summary">Summary</h3>
<ul>
<li>First a demo in Agda to show how that system feels
<ul>
<li>Building the <code>filter</code> function as a warm-up</li>
</ul></li>
<li>Next doing it to Haskell with ghc-mod
<ul>
<li>Rewriting <code>filter</code> to compare</li>
<li>That was easy, so let’s try functions that are not intuitively clear</li>
<li>like deriving the Functor instance for the free monad</li>
<li>…and then the Applicative instance</li>
<li>Success! Look how hard that would have been without help</li>
</ul></li>
</ul>
</div>
]]></summary>
</entry>
<entry>
    <title>Virtualizing a Hadoop Cluster (two videos)</title>
    <link href="http://begriffs.com/posts/2015-01-28-virtualizing-hadoop.html" />
    <id>http://begriffs.com/posts/2015-01-28-virtualizing-hadoop.html</id>
    <published>2015-01-28T00:00:00Z</published>
    <updated>2015-01-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Virtualizing a Hadoop Cluster (two videos)</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">January 28, 2015</h5>
</div>

<div class="content">
  <p><a href="http://joelburget.com/">Tom Phelan</a>, Chief Architect at <a href="https://www.linkedin.com/pub/tom-phelan/0/749/61b">BlueData</a> talks about the appropriate situations in which to virtualize Hadoop, either in containers or in virtual machines. In evaluating the situations he explains what questions you should and should not be asking.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/117907449.hd.mp4?s=de0ba4566372c0f6ae14d8bb51f50497"
         poster="https://i.vimeocdn.com/video/504771461.png?mw=700"
  ></video>
</div>
<h3 id="summary">Summary</h3>
<ul>
<li>What BlueData has learned about running Hadoop jobs</li>
<li>Under what situations should one virtualize a Hadoop cluster</li>
<li>The shape and components of a physical cluster
<ul>
<li>Both master and worker controllers contain</li>
<li>Disks, server, named node, and resource manager</li>
</ul></li>
<li>Types of virtualization:
<ul>
<li>public cloud</li>
<li>private cloud / hypervisor (strong fault isolation)</li>
<li>private cloud / containers (weak fault isolation)</li>
<li>paravirtualization</li>
</ul></li>
<li>The appropriate infrastructure for various situations
<ul>
<li>Questions NOT to ask</li>
<li>Questions to ask</li>
</ul></li>
<li>Performance and Data Locality</li>
<li>Five use-cases and their infrastructure needs</li>
</ul>
<h3 id="pt-2-orchestration-with-docker">Pt 2: Orchestration with Docker</h3>
<p><a href="https://twitter.com/joel_k_baxter">Joel Baxter</a>, also at BlueData, leads a breakout session about what an ideal orchestration manager would look like for managing Hadoop clusters and associated data. The state of the art is evolving but not there yet.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/118047022.hd.mp4?s=91cfdf09dcdfcc374d31c69c3008ce3a"
         poster="https://i.vimeocdn.com/video/504974721.png?mw=700"
  ></video>
</div>
<h3 id="summary-1">Summary</h3>
<ul>
<li>Hadoop Cluster orchestration with Docker</li>
<li>Wishlist for an ideal orchestration manager
<ul>
<li>Add/remove a host from a list of peers</li>
<li>Track resource consumption/availability per host</li>
<li>Library of pre-prepared application images</li>
<li>Multi-tenant protection
<ul>
<li>Performance isolation and data security</li>
<li>Containers are less secure than virtual machines</li>
<li>Quotas and priority</li>
</ul></li>
<li>Matching containers with physical hosts</li>
<li>Spreading containers across fault-zones</li>
<li>Packing containers in contiguous memory</li>
</ul></li>
<li>Migrations vs stateless servers</li>
<li>Circumventing docker and doing IP address allocation</li>
<li>DNS settings for bidirectional Hadoop communication</li>
<li>Wishlist for storage
<ul>
<li>Docker volumes</li>
<li>Volumes for fragments of HDFS are hard to reconstruct on container resurrection</li>
<li>Ship your data to another db, or paravirtualize the storage</li>
</ul></li>
<li>Solutions?
<ul>
<li>There is not yet a turn-key orchestration system that solves all the items in the wishlists</li>
<li>The area is rapidly evolving</li>
</ul></li>
</ul>
</div>
]]></summary>
</entry>
<entry>
    <title>Writing a React JS front-end in Haskell</title>
    <link href="http://begriffs.com/posts/2015-01-12-reactjs-in-haskell.html" />
    <id>http://begriffs.com/posts/2015-01-12-reactjs-in-haskell.html</id>
    <published>2015-01-12T00:00:00Z</published>
    <updated>2015-01-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post-header">
  <h3>Writing a React JS front-end in Haskell</h3>
  <a href="https://twitter.com/share" class="twitter-share-button pull-right"
     data-via="begriffs" data-count="none">Tweet</a>
  <h5 class="text-muted">January 12, 2015</h5>
</div>

<div class="content">
  <p><a href="http://joelburget.com/">Joel Burget</a>, Developer at <a href="https://www.khanacademy.org/">Khan Academy</a> explains the design of his new <a href="https://hackage.haskell.org/package/react-haskell">react-haskell</a> library.</p>
<p>It allows you to write a front-end app in Haskell which you compile to JavaScript via Haste and render using React JS.</p>
<div class="flowplayer" data-embed="false">
<video src="http://player.vimeo.com/external/116516127.hd.mp4?s=69cd6b66fe5bff69a43e1b4375482edc"
         poster="https://i.vimeocdn.com/video/502970736.png?mw=700"
  ></video>
</div>
<p>Check out the code at <a href="http://joelburget.com/react-haskell/">joelburget.com/react-haskell</a>.</p>
<h3 id="overview">Overview</h3>
<ul>
<li>Overview of React itself (without the Haskell)
<ul>
<li>The virtual dom</li>
<li>Partial diffing</li>
</ul></li>
<li>Haste: turning Haskell into JavaScript</li>
<li>Inspiration for React-Haskell
<ul>
<li>Blaze-HTML builder provided the general flavor of the API</li>
<li>Recently Joel has been switching to the Lucid library instead of Blaze</li>
<li>Building the eventual DOM happens inside a monad</li>
<li>It could be considered an abuse of a monad, but provides nice do-notation</li>
</ul></li>
<li>Examples of react html written in Haskell</li>
<li>Event handlers</li>
<li>Principles of interruptible animations (inspired by UIKit)
<ul>
<li>the model is discrete</li>
<li>we apply updates immediately</li>
<li>use additive animation by default</li>
</ul></li>
<li>Performance observations</li>
</ul>
</div>
]]></summary>
</entry>

</feed>
