---
title: Minimal Instruction Set
---

Processors include many optimization tricks, but I wonder if plain
simplicity would be the fastest design of all. The traditional
computer design presupposes a procedural, numerical paradigm. We
are given registers which hold numbers, which index memory cells,
etc. This arithmetic bias seems a roundabout way to achieve Turing
completeness, sort of like working with logical statements using
only their GÃ¶del numbers.

I wonder if the classical Von Neumann architecture arose from the
schoolboy viewpoint of "math is numbers." Early education stresses
brute manipulation rather than deductive mathematical structure and
might have predisposed us to design computers the way we do.

Since modern programmers are far removed from the abstraction of
machine code, why do we bother to make it "helpful?" We include a
host of arithmetic operations at the cost of complicating the basic
hardware. Why not let arithmetic be defined in a higher level
language? Can brute hardware be reduced to some single simple
principle, like propositional logic operators can be reduced to the
Sheffer stroke? I am aware that circuits are reduced to NAND gates
in this way, but you understand my analogy.

If we find that full computation can be based off one simple
operation, then we can search for a sort of isomorphism of this
operation in physics. Find a simple movement of molecules or energy
which embodies our operation and which happens so quickly that its
speed makes up for the amount of these small operations it takes
to do significant calculations.

What might we choose for our operation? Consider combinatory logic
which can code lambda calculus without the inconvenience of variable
binding.
